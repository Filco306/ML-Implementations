#' crossValidationSplits
#'
#' Splits the data into k partitions, which can later be used together with the function "CVtrainAndTestSet"
#'
#' @param dataSet is the set of data to split up into. Make sure samples are the rows and the columns are the features.
#' @param k is the number to splits to partition the data into.
#' @param seed is the seed used for the randomization. Default to null
#' @keywords Cross validation
#' @export
#' @examples
#'
#'
#'
crossValidationSplits <- function(dataSet, k, seed = NULL) {
  smp_size = floor(nrow(dataSet)/k)

  if (!is.null(seed)) {
    ## set the seed to make your partition reproducible
    set.seed(seed)
  }


  folds <- list()

  for (i in 1:k) {
    newFold <- sample(seq_len(nrow(dataSet)), size = smp_size)
    folds[[i]] = data.frame(dataSet[newFold,])
    dataSet <- dataSet[-newFold,]
  }

  # If there are still samples left in the original dataset.
  if (nrow(dataSet) != 0) {
    folds[[i]] = rbind(folds[[i]], dataSet)
  }

  return(folds)

}


#' trainTestSplit
#'
#' Splits the data into a training and test set
#'
#' @param dataSet is the set of data to split up. Assumes data set does not contain labels but is split up separately.
#' @param labels is a vector containing the labels for the dataSet. Needs to have the same order and length as dataSet (of course).
#' @param testSetFraction
#' @keywords Cross validation
#' @export
#' @examples
#'
#'
#'

trainTestSplit <- function(dataSet, labels, testSetFraction, seed = NULL) {

  smp_size <- floor((1 - testSetFraction) * nrow(dataSet))

  if (!is.null(seed)) {
  ## set the seed to make your partition reproducible
    set.seed(seed)
  }
  train_ind <- sample(seq_len(nrow(dataSet)), size = smp_size)

  train <- dataSet[train_ind, ]
  test <- dataSet[-train_ind, ]

  trainLabels = labels[train_ind]
  testLabels = labels[-train_ind]
  normParam <- preProcess(train)

  train = predict(normParam,train)
  test = predict(normParam, test)

  train$label = NULL
  test$label = NULL

  train = matrix(c(train$x,train$y), nrow = nrow(train), ncol = 2)
  test = matrix(c(test$x, test$y), nrow = nrow(test), ncol = 2)

  return(list(test, train, trainLabels, testLabels))
}


#' CVtrainAndValidationSet
#'
#' Returns a training set (k-1 folds) and a validation set (1 fold) in a list generated by the function \code{crossValidationSplits}.
#'
#' @param splits List of partitions of the dataset. Obtained in correct format through function crossValidationSplits.
#' @param splitIndex Instructs which partition to take. If it is -1, it will just take a random split. Otherwise, it will take the corresponding index in the list.
#' @param stdDev to standardize the vector with. If nothing is sent in, the sd of the feature vector will be used.
#' @keywords Cross Validation
#' @export
#' @examples
#'
#'
#'
CVtrainAndValidationSet <- function(splits, splitIndex = -1) {


  if (splitIndex == -1 || splitIndex > length(splits)) {
    testIndex = floor(runif(1, min = 1, max = length(splits)))
    test = splits[[testIndex]]
    train = splits
    train[[testIndex]] = NULL
  } else {
    test = splits[[splitIndex]]
    train = splits
    train[[splitIndex]] = NULL
  }
  trainSet = train[[1]]
  for (i in 2:length(train)) {
    trainSet = rbind(trainSet, train[[i]])
  }

  return(list(trainSet,test))
}

#' standardizeFeatures
#'
#' Standardizes a vector of features with mu and sd. Not sure yet if correct, not validated yet.
#'
#' @param feature Vector of features to standardize according to its length
#' @param mu to standardize the vector with. If nothing is sent in, the mean of the feature vector will be used.
#' @param stdDev to standardize the vector with. If nothing is sent in, the sd of the feature vector will be used.
#' @keywords standardization
#' @export
#' @examples
#'
#'
#'
standardizeFeature <- function(feature, mu = NULL, stdDev = NULL) {

  if (is.null(mu) & is.null(stdDev)) {
    feat = (feature - mean(feature))/sd(feature)
  } else {
    feat = (feature - mu)/stdDev
  }
  return(feat)
}

#' rescaleFeature
#'
#' Standardizes a vector of features with mu and sd. Not sure yet if correct, not validated yet.
#'
#' @param feature Vector of scaled features to rescale
#' @param mu to rescale the vector with.
#' @param stdDev to rescale the vector with.
#' @keywords standardization
#' @export
#' @examples
#'
#'
#'
rescaleFeature = function(feature, mu, stdDev) {
  return(stdDev*scaledData + avg)
}

#' GenerateClusters
#'
#' Generates clusters. In other words, generates clusters out of sampling from a normal distribution, in N dimensions. The number in each cluster is also sampled from a normal distribution.
#'
#' @param avgSampPerCluster is the expected number of datapoints in one cluster
#' @param clusterVar is the variance used to sample the number of particles in each cluster.
#' @param nrClusters is the number of clusters desired.
#' @param featureMins is an N-dimensional vector, containing the smallest possible value for each feature
#' @param featuresMaxes is an N-dimensional vector, containing the biggest possible value for each feature
#' @keywords cluster
#' @export
#' @examples
#'
#'
#'
GenerateClusters = function(avgSampPerCluster, clusterVar, nrClusters, featureMins, featureMaxes, seed = NULL) {
  if (!is.null(seed)) {
    set.seed(seed)
  }

  df = data.frame(matrix(rep(0,length(featureMins)), ncol = length(featureMins), nrow = 1))
  names = colnames(df)
  for (i in 1:nrClusters) {
    means = as.numeric()
    vars = as.numeric()
    for (j in 1:length(featureMins)) {
      means[j] = runif(1, min = featureMins[j], max = featureMaxes[j])
      vars[j] = abs(featureMins[j] - featureMaxes[j])
    }
    nrInCluster = floor(rnorm(1, avgSampPerCluster, clusterVar))
    cluster = rmvnorm(nrInCluster, mean = means, sigma = diag(vars))
    colnames(cluster) = names
    df = rbind(df, cluster)
  }
  df = df[-c(1),]
  return(df)
}



#' Standardize data
#'
#' Standardize all data in a data frame or a matrix that are of type double. Assumes normal distribution on data.
#'
#' @param dataIn is a matrix or a dataframe sent in to be standardized.
#' @param typeOut specifies whether we want a dataframe or matrix out
#' @keywords cluster
#' @export
#' @examples
#'
#'
#'
standardizeData = function(dataIn, typeOut = "dataframe") {

  names = colnames(dataIn)

  dataframe = apply(dataIn, 2, function(col) {
    if (typeof(col) == "double") {
      return(standardizeFeature(col))
    } else {
      return(col)
    }

  })

  if (typeOut == "dataframe") {
    dataframe = data.frame(dataframe)
  }
  colnames(dataframe) = names
  return(dataframe)
}

#' True positive rate
#'
#' Calculates the true positive rate for predictions derived from a probability threshold, given predictions and labels. Send in labels and predictions in factor form.
#'
#' @param predictions is the vector of predictions, with true labels sent in as 1, false as 0
#' @param labels is the vector of actual labels, with true labels sent in as 1, false as 0
#' @keywords TPR
#' @export
#' @examples
#'
#'
#'
TPR = function(predictions, labels) {
  if(length(levels(predictions)) == 1) {
    predictions = as.numeric(predictions)
    predictions = as.factor(rep(levels(labels)[predictions[1]+1],length(predictions)))
  } else {
    levels(predictions) = levels(labels)
  }

  y = labels
  y_hat = predictions
  conf_m = table(Actual = y, Predicted = y_hat)
  # True positive rate - true positives divided by all actually being positive
  if (dim(conf_m)[2] == 1 && levels(labels)[2] %in% y_hat) {
    return(1) # If we only have good, all are classified as positive correctly (although many are predicted incorrectly) and this is thus 1
  } else if (dim(conf_m)[2] == 1 && levels(labels)[1] %in% y_hat) {
    return(0)
  } else {
    TP = conf_m[2,2] # True positives
    N_plus = sum(conf_m[2,])
    return(TP/N_plus)
  }

}




#' True positive rate
#'
#' Calculates the false positive rate for predictions derived from a probability threshold, given predictions and labels. Send in labels and predictions in factor form.
#'
#' @param predictions is the vector of predictions, with true labels sent in as 1, false as 0
#' @param labels is the vector of actual labels, with true labels sent in as 1, false as 0
#' @keywords FPR
#' @export
#' @examples
#'
#'
#'
FPR = function(predictions, labels) {
  if(length(levels(predictions)) == 1) {
    predictions = as.numeric(predictions)
    predictions = as.factor(rep(levels(labels)[predictions[1]+1],length(predictions)))
  } else {
    levels(predictions) = levels(labels)
  }
  y = labels
  y_hat = predictions
  conf_m = table(Actual = y, Predicted = y_hat)
  # False positive rate - false positives divided by all actually being negative
  if (dim(conf_m)[2] == 1 && levels(labels)[2] %in% y_hat) {
    return(1) # If we only have "good", it means all are classified as positive, and the false positives is thus as many as the number of false ones. Thus, the FP/N_minus = 1
  } else if (dim(conf_m)[2] == 1 && levels(labels)[1] %in% y_hat) {
    return(0) # If we only have bad, we do not have a single false positive as there are no positives. Hence, 0
  } else {
    FP = conf_m[1,2]
    N_minus = sum(conf_m[1,])
    return(FP/N_minus)
  }

}


#' Area Under Curve
#'
#' Calculates the area under the curve.
#'
#' @param TPR is a vector of true positive rates.
#' @param FPR is a vector of the false positive rates.
#' @keywords AUC
#' @export
#' @examples
#'
#'
#'
AUC = function(TPR, FPR) {
  # TPR is y, FPR is x
  # Order after FPR
  xInd = order(FPR)
  x = FPR[xInd]
  y = TPR[xInd]
  area = 0
  for (i in 2:length(TPR)) {
    area = (x[i]-x[i-1])*(y[i] + y[i-1])/2 + area
  }
  return(area)
}



#' Classification rate
#'
#' Calculates, out of a confusion matrix, the amount of correctly classified data points.
#'
#' @param confMatrix is the confusion matrix to calculate the classification rate for.
#' @keywords confusion matrix, accuracy
#' @export
#' @examples
#'
#'
#'
classificationRate = function(confMatrix) {
  return(sum(diag(confMatrix))/sum(confMatrix))
}

#' MSE - Mean Squared Error
#'
#' Calculates the mean squared error for predictions.
#'
#' @param y is a vector containing the actual response values.
#' @param y_hat is a vector containing the predictions.
#' @keywords error
#' @export
#' @examples
#'
#'
#'
MSE = function(y,y_hat) {
  return(mean((y - y_hat)^2))
}

#' Get formula
#'
#' Returns the formula as a string
#'
#' @param response is the name of the
#' @param features is a vector containing the names of the features
#' @keywords error
#' @export
#' @examples
#' get_formula("Fertility", c("Infant.Mortality", "No.In.Household"))
#' # Will print out "Fertility ~ Infant.Mortality + No.In.Household"
get_formula = function(response, features) {
  f = paste(c(response, paste(features, collapse = ' + ')), collapse = " ~ ")
}

#' Holdout method
#'
#' Returns a training and a test set.
#'
#' @param data is the data to split
#' @param train_partition is the percentual size of the data set. 0.75 means 75 % of the dataset will be trained on.
#' @param seed is the seed to set. Default set to NULL
#' @keywords error
#' @export
holdout_method = function(data, train_partition = 0.75, seed = NULL) {
  if (!is.null(seed)) {
    set.seed(seed)
  }
  train_ids = sample(1:nrow(data), size = nrow(data)*train_partition, replace = FALSE)
  train = data[train_ids,]
  test = data[-train_ids,]
  return(list(train,test))
}

#' Classification with loss
#'
#' Function used for binary classification to incorporate loss.
#'
#' @param FP_punishment is the value in the loss function for all the false positives yielded.
#' @param FP_punishment is the value in the loss function for all the false negatives yielded.
#' @param probabilites is a vector or matrix with the probabilities. Can either only be the probabilites for a positive result, or a Nx2 matrix with the probability for a positive classification in the second column.
#' @keywords loss, matrix, binary, classification
#' @export
loss_binary_classification = function(FP_punishment, FN_punishment, probabilities) {
  if (dim(probabilities)[2] == 2) {
    probs_positive = probabilities[,2]
  } else {
    probs_positive = probabilities
  }
  return(apply(as.matrix(probs_positive, 1, function(row) {
    losses = c(FN_punishment*row, FP_punishment*(1 - row))
  })))
}

#' Robustness loss binary classification
#'
#' Function used for binary classification to measure quality of the predictions. Gives an indication of the "robustness" of the model, i.e. if the model is in general confident on its predictions or the probabilities are rather centered around 0.5, making it a worse predictive model.
#'
#' @param probs_positive are the probabilities for a positive classification result, yielded by the model.
#' @param y are the actual predictions. Can be sent in as factors, but preferred that they are converted to 1 for positive, 0 for negative.
#' @keywords loss, robustness, binary, classification
#' @export
robust_loss_binary_classification = function(probs_positive, y) {
  labels = as.numeric(y)
  y = ifelse(y == max(labels), 1, 0)
  return(sum(y*log(probs_positive) + (1-y)*log(1 - probs_positive)))
}


#' Neural network (ANN)
#'
#' Creates a fully connected ANN which can be trained will be trained with backpropagation.
#' Not done implemented. Remains TBD.
#'
#' @param x are the features used in the neural network. Send in as a matrix or dataframe.
#' @param y are the response variables.
#' @param hidden_layers are the hidden layers in the network. Send in as a vector consisting of numbers, declaring the number of neurones in each hidden layer.
#' @param activation_functions are sent in as a vector of strings, with as many activation functions as layers specified plus 1 for the final layer. If only one activation function is specified, this will be used throughout the whole network. Default is rectified linear units.
#' @param weights_init vector with initial weights.
#' @param verbose boolean should be outputted while training. Default set to false.
#' @param threshold is the threshold of the error when to stop training.
#' @keywords artificial, neural, network, backpropagation, ANN, backpropagation
#' @export
neural_network = function(x, y, hidden_layers, activation_functions = "relu", task = "regression", weights_init = NULL) {
 x = as.matrix(x)
 y = as.matrix(y)

 # Calculate the total number of weights. Should be ((nr_feats+1)*nr_neurones_first_layer + (nr_neurones_first_layer+1)*nr_neurones_second_layer) + etc etc
 nr_weights = 0
 nr_feats = ncol(x)
 if (!is.null(weights_init)) {
   weights = weights_init
 } else {

 }


 # Will return the neural network as an object.
 return(neural_net)
}

#' Substitute values
#'
#' Substitutes values in a column with other given values.
#' @param column is the vector of values from which the values should be replaced.
#' @param ranking are the values that are to be replaced in column. Should contain all possible values in the matrix.
#' @param corresponding are the corresponding values that replaces the values given in ranking. Order is important.
#' @param null_val is a value given as to what replace null values with.
#' @keywords data, manipulation
#' @export
substitute_values = function(column, ranking, corresponding, null_val = NULL) {
  return(apply(as.matrix(column), 1, function(val) {
    if(is.na(val)) {
      return(null_val)
    } else {
      return(corresponding[which(ranking == val)])
    }
  }))
}

#' Create dummy variables
#'
#' Creates dummy variables for a specific column in a data frame. Returns a data frame with the old column gone and a new with all the dummy variables.
#' @param data is the data frame (send in the whole data frame)
#' @param col_name is the name of the column, given as a string. It can also be given as an index.
#' @param NA_val is what to impute on the NA values. Default NA (no imputation made).
#' @keywords dummy, data, manipulation
#' @export
create_dummy_vars = function(data, col_name, NA_val = NA) {
  col = data[, col_name]
  l = levels(col)
  # Create dummy var matrix
  dummy_vars = matrix(NA, nrow = length(col), ncol = length(l))
  colnames(dummy_vars) = l
  for (i in 1:length(l)) {
    dummy_vars[,i] = as.integer(ifelse(as.character(col) == l[i], 1, 0))
  }
  if (!is.na(NA_val)) {
    dummy_vars[is.na(dummy_vars)] = NA_val
  }
  data = cbind(data[, !(colnames(data) %in% col_name)], data.frame(dummy_vars))
  return(data)
}

#' K-fold cross-validation
#'
#' Performs cross-validation on an arbitrary model in an arbitrary way.
#'
#' By default, if \code{fold_indices = NULL}, k-fold-CV is performed.
#'
#' Not tested yet.
#' @param model is the model function sent in onto which the cross-validation should be performed.
#' @param data is the data given to split up
#' @param error_measure_function is the error function with which one measures the generalization error.
#' @param formula_ is the formula to pass to the model function. \code{NULL} by default, and if it is \code{NULL}, \code{target} and \code{predictors} must be passed.
#' @param target is the name of the target variable. Is NULL by default under assumption that formula exists.
#' @param predictors is a vector of the names of the predictors. Is NULL by default under assumption that formula exists.
#' @param fold_indices is a list of fold indices. Default is \code{NULL}, meaning that the partitioning will be randomized into \code{n_folds}.
#' @param n_folds is the number of folds to divide.
#' @param ... are additional model parameters.
#' @export
k_fold_cv = function(model, data, error_measure_function, formula_ = NULL, target = NULL, predictors = NULL, fold_indices = NULL, n_folds = 5, seed = 123, ...) {



  if (is.null(fold_indices)) {
    # Then we create the folds!
    data_splits = crossValidationSplits(data, k = n_folds, seed = seed)

  }

  error = as.numeric()
  for (i in 1:n_folds) {
    train_validation = CVtrainAndValidationSet(data_splits, splitIndex = i)
    train = train_validation[[1]]
    validation = train_validation[[2]]

    if (!is.null(formula_)) {
      fit = model(formula_, train,...)
      error[i] = error_measure_function(validation[,target],predict(fit,validation))
    } else {
      fit = model(x=train[,predictors],y=train[,target],...)
      error[i] = error_measure_function(validation[,target],predict(fit,validation[,predictors]))
    }

  }

  cv_results = list()
  if (!is.null(formula_)) {
    cv_results$formula = formula_
  } else {
    cv_results$formula = get_formula(target, predictors)
  }

  cv_results$error = error
  cv_results$avg_error = mean(error)
  cv_results$folds = data_splits

  return(cv_results)
}


#' Time-series cross-validation
#'
#' Performs time-series cross-validation on a model.
#'
#' Not tested yet.
#' @param model is the model on which to perform the time-series cross-validation.
#' @param data is the data on which to perform the cross-validation and provide good measures for.
#' @param formula_ is the formula to pass to the model function. \code{NULL} by default, and if it is \code{NULL}, \code{target} and \code{predictors} must be passed.
#' @param target is the name of the target variable. Can be \code{NULL} if \code{formula_} is specified.
#' @param predictors is a vector of the names of the predictors. Can be \code{NULL} if \code{formula_} is specified.
#' @param start_index is the first index to perform the CV from. Default is 3 (not reasonable to start from 2 and forward).
#' @param error_measure_function is the error function with which one measures the generalization error.
#' @param ... are additional model parameters.
#' @export
time_series_cv = function(model, data, formula_ = NULL, target = NULL, predictors = NULL, start_index = 3, error_measure_function, ...) {


  error = as.numeric()

  for (i in start_index:nrow(data)) {
    train = data[1:(i-1),]
    validation = data[i,]

    if (!is.null(formula_)) {
      fit = model(formula_, train,...)
      error[i] = error_measure_function(validation[,target],predict(fit,validation))
    } else {
      fit = model(x=train[,predictors],y=train[,target],...)
      error[i] = error_measure_function(validation[,target],predict(fit,validation[,predictors]))
    }

  }
  cv_results = list()
  if (!is.null(formula_)) {
    cv_results$formula = formula_
  } else {
    cv_results$formula = get_formula(target, predictors)
  }

  cv_results$error = error
  cv_results$avg_error = mean(error)
  cv_results$folds = data_splits

  return(cv_results)

}


#' Leave-one-out Cross-Validation
#'
#' Performs Leave-One-Out Cross-validation on the dataset given.
#'
#' Not tested yet.
#' @param model is the model on which to perform the time-series cross-validation.
#' @param data is the data on which to perform the cross-validation and provide good measures for.
#' @param formula_ is the formula to pass to the model function. \code{NULL} by default, and if it is \code{NULL}, \code{target} and \code{predictors} must be passed.
#' @param target is the name of the target variable. Can be \code{NULL} if \code{formula_} is specified.
#' @param predictors is a vector of the names of the predictors. Can be \code{NULL} if \code{formula_} is specified.
#' @param error_measure_function is the error function with which one measures the generalization error.
#' @param ... are additional model parameters.
#' @export
leave_one_out_cv = function(model, data, formula_, target = NULL, predictors = NULL, error_measure_function, ...) {
  error = as.numeric()

  for (i in 1:nrow(data)) {
    train = data[-i,]
    validation = data[i,]

    if (!is.null(formula_)) {
      fit = model(formula_, train,...)
      error[i] = error_measure_function(validation[,target],predict(fit,validation))
    } else {
      fit = model(x=train[,predictors],y=train[,target],...)
      error[i] = error_measure_function(validation[,target],predict(fit,validation[,predictors]))
    }

  }
  cv_results = list()
  if (!is.null(formula_)) {
    cv_results$formula = formula_
  } else {
    cv_results$formula = get_formula(target, predictors)
  }

  cv_results$error = error
  cv_results$avg_error = mean(error)
  cv_results$folds = data_splits

  return(cv_results)
}


#' Distribution plot
#'
#' Plots the distribution of data.
#' @param x is the vector of data to plot.
#' @param ... are additional parameters to the \code{hist} function.
#' @export
dist_plot = function(x, ...) {
  hist(x, freq = FALSE, ...)
  lines(density(x), lwd = 4, col = "red")
}


#' Get skewness
#'
#' Get the skewness of all variables in a data set.
#' @param data is the data frame for which to obtain the skews.
#' @param skewed_threshold is the absolute threshold value. Default is 1, meaning all variable having \code{abs(skewness(variable)) > 1} will be returned.
#' @export
get_skewed_vars = function(data, skewed_threshold = 1) {
  library(e1071)
  is_not_bin = c()
  for (i in 1:ncol(data)) {
    if (any(!(names(table(data[,i])) %in% c("0","1"))) & abs(skewness(data[!is.na(data[,i]),i])) > skewed_threshold) {
      is_not_bin[i] = skewness(data[,i])
      names(is_not_bin)[i] = colnames(data)[i]
    }
  }

  is_not_bin = is_not_bin[!is.na(is_not_bin)]
  return(is_not_bin)
}

#' Plot imputation effect
#'
#' Plots a what-if scenario and compares the scenarios what the distribution would look like if
#' @param feature is the feature to impute for.
#' @param impute_func is the function to use for imputation. Takes in, by default, all the non-NA feature values.
#' @param ... are additional values to \code{impute_func}.
#' @export
plot_imputation_effect = function(feature, impute_func = median,...) {
  par(mfrow = c(2,1))
  hist(feature, freq = F, ylim = c(0, max(density(feature[!is.na(feature)])$y)), col = "blue", breaks = 100, main = "Before imputing median to null vals")
  lines(density(feature[!is.na(feature)]), lwd = 3, col = "red")
  abline(v = median(feature[!is.na(feature)]), lwd = 3, col = "green")
  feature[is.na(feature)] = impute_func(feature[!is.na(feature)],...)
  hist(feature, freq = F, ylim = c(0, max(density(feature[!is.na(feature)])$y)), col = "blue", breaks = 100, main = "After imputing median to null vals")
  lines(density(feature[!is.na(feature)]), lwd = 3, col = "red")
  abline(v = median(feature[!is.na(feature)]), lwd = 3, col = "green")
  par(mfrow = c(1,1))
}


#' Replacer
#'
#' Replaces values in a a vector \code{column} with values in a corresponding vector \code{ranking}.
#' @param column is the vector to replace values for.
#' @param ranking
#' @param corresponding is the vector with values to replace.
#' @param NA_val is the value to replace \code{NA}-values with. Default is \code{NA}, meaning they are not replaced.
#' @export
replace_vals = function(column, ranking, corresponding, NA_val = NA) {
  return(apply(as.matrix(column), 1, function(val) {
    if(is.na(val)) {
      return(NA_val)
    } else if (val %in% ranking) {
      return(corresponding[which(ranking == val)])
    } else {
      warning(paste("Note: Value",val,"not found in corresponding vector."))
      return(val)
    }
  }))
}
