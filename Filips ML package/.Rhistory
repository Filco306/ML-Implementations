avgBestArmUCB = bestArmPlayed(UCBResults$ArmChosen, mus)
avgBestArmSM1 = bestArmPlayed(softMaxC1$ArmChosen, mus)
avgBestArmSM10 = bestArmPlayed(softMaxC10$ArmChosen, mus)
} else {
avgRegretEps1 = avgRegretEps1*(i-1)/i + calc_regret(mus[which.max(mus)], epsilonResults1$ArmChosen, mus)/i
avgRegretEps2 = avgRegretEps2*(i-1)/i + calc_regret(mus[which.max(mus)], epsilonResults2$ArmChosen, mus)/i
avgRegretEps3 = avgRegretEps3*(i-1)/i + calc_regret(mus[which.max(mus)], epsilonResults3$ArmChosen, mus)/i
avgRegretUCB = avgRegretUCB*(i-1)/i + calc_regret(mus[which.max(mus)], UCBResults$ArmChosen, mus)/i
avgRegretSM1 = avgRegretSM1*(i-1)/i + calc_regret(mus[which.max(mus)], softMaxC1$ArmChosen, mus)/i
avgRegretSM10 = avgRegretSM10*(i-1)/i + calc_regret(mus[which.max(mus)], softMaxC10$ArmChosen, mus)/i
avgBestArmEps1 = avgBestArmEps1*(i-1)/i + bestArmPlayed(epsilonResults1$ArmChosen,mus)/i
avgBestArmEps2 = avgBestArmEps2*(i-1)/i + bestArmPlayed(epsilonResults2$ArmChosen,mus)/i
avgBestArmEps3 = avgBestArmEps3*(i-1)/i + bestArmPlayed(epsilonResults3$ArmChosen,mus)/i
avgBestArmUCB = avgBestArmUCB*(i-1)/i + bestArmPlayed(UCBResults$ArmChosen,mus)/i
avgBestArmSM1 = avgBestArmSM1*(i-1)/i + bestArmPlayed(softMaxC1$ArmChosen,mus)/i
avgBestArmSM10 = avgBestArmSM10*(i-1)/i + bestArmPlayed(softMaxC10$ArmChosen,mus)/i
}
print(i)
}
#results = analyzeResults(k, mus, sig, epsilonResults, UCBResults)
analyze(k, sqrt(sig), avgRegretEps1, avgRegretEps2, avgRegretEps3, avgRegretUCB, avgRegretSM1, avgRegretSM10, avgBestArmEps1, avgBestArmEps2, avgBestArmEps3, avgBestArmUCB, avgBestArmSM1, avgBestArmSM10)
}
}
analyze(k, sqrt(sig), avgRegretEps1, avgRegretEps2, avgRegretEps3, avgRegretUCB, avgRegretSM1, avgRegretSM10, avgBestArmEps1, avgBestArmEps2, avgBestArmEps3, avgBestArmUCB, avgBestArmSM1, avgBestArmSM10)
help(legend)
####################################
######## Initialize functions ######
####################################
Get_Convergence = function(mus) {
sortedMaxRewards = sort(mus, decreasing = TRUE)
return(sortedMaxRewards[1] - sortedMaxRewards[2])
}
normalize = function(vec) {
print(sd(vec))
if (sd(vec) != 0) {
return((vec - mean(vec)/sd(vec)))
} else {
return(vec)
}
}
Convergence_Means <- function (samples) {
means <- numeric()
for (i in 1:length(samples)) {
means[i] <- mean(samples[1:i])
}
return(means)
}
ret_epsilon = function(mode = 1, t = NULL, c = 1) {
if (mode == 1) {
return(0.5)
} else if (mode == 2) {
return(min(1,c/sqrt(t)))
} else if (mode == 3) {
return(min(1,c/log(t)))
} else if (mode == 4) {
# Another suggestion?
}
}
epsilonGreedy = function(k, mus, sigma2) {
epsilon = ret_epsilon(mode = 1)
# Set all mu estimates to 0.5 first.
muEstimates = rep(0.5, k)
# Set probability to pick each arm to epsilon/k,
probabilities = rep(epsilon/k, k)
# Set the best arm's probability to 1 - epsilon + e/k, i.e. add 1 - epsilon
# In this case, all
# Initialize vectors to store information
rewards = as.numeric()
armChosen = as.numeric()
t = 1
while (t <= T) {
# update epsilon
epsilon = ret_epsilon(epsilonMode, t = t)
# Sample an arm
armChosen[t] = sample(seq(1, k, 1), size = 1, prob = probabilities)
rewards[t] = rnorm(n=1,mean = mus[armChosen[t]], sd = sqrt(sigma2))
# Update muEstimates
muEstimates[armChosen[t]] = sum(rewards[armChosen == armChosen[t]])/sum(armChosen == armChosen[t])
# Update probabilities
probabilities = rep(epsilon/k, k)
# Set the best arm's probability to 1 - epsilon + e/k, i.e. add 1 - epsilon
probabilities[which.max(muEstimates)] = probabilities[which.max(muEstimates)] + 1 - epsilon
t = t + 1
}
data = data.frame(armChosen, rewards)
colnames(data) = c("ArmChosen", "Rewards")
return(data)
}
SoftMax = function(k, mus, sigma2, c = NULL) {
muEstimates = rep(0.5, k)
# Initialize vectors to store information
rewards = as.numeric()
armChosen = as.numeric()
n_k_t = rep(0,k)
for (t in 1:T) {
if (!is.null(c)) {
eta_t = c/t
} else {
eta_t = 0.9
}
probs = exp(eta_t*muEstimates)
if(any(is.na(probs))) {
print(probs)
print(muEstimates)
muEstimates = normalize(muEstimates)
print(muEstimates)
probs = exp(eta_t*muEstimates)/sum(exp(eta_t*muEstimates))
print(probs)
}
armChosen[t] = sample(seq(1,k,1), size = 1, prob = probs)
rewards[t] = rnorm(n=1,mean = mus[armChosen[t]], sd = sqrt(sigma2))
n_k_t[armChosen[t]] = n_k_t[armChosen[t]] + 1
muEstimates[armChosen[t]] = sum(rewards[armChosen == armChosen[t]])/(n_k_t[armChosen[t]] + max(0, 1 - n_k_t[armChosen[t]]))
}
data = data.frame(armChosen, rewards)
colnames(data) = c("ArmChosen", "Rewards")
return(data)
}
UCB = function(k, mus, sigma2) {
muEstimates = rep(0.5, k)
# Initialize vectors to store information
rewards = as.numeric()
armChosen = as.numeric()
n_k_t = rep(0,k)
t = 1
while (t <= T) {
# Generate the ucbs. We need the indices for this, hence the for loop.
ucbs = as.numeric()
for (i in 1:k) {
ucbs[i] = muEstimates[i] + sqrt(2*log(t)/(n_k_t[i] + max(0, 1 - n_k_t[i])))
}
# Choose arm from the one with highest UCB
armChosen[t] = which.max(ucbs)
# update n_k_t
n_k_t[armChosen[t]] = n_k_t[armChosen[t]] + 1
# Sample reward
rewards[t] = rnorm(n=1, mean = mus[armChosen[t]], sd = sqrt(sigma2))
# Update muEstimates
muEstimates[armChosen[t]] = sum(rewards[armChosen == armChosen[t]])/(n_k_t[armChosen[t]] + max(0, 1 - n_k_t[armChosen[t]]))
t = t + 1
}
data = data.frame(armChosen, rewards)
colnames(data) = c("ArmChosen", "Rewards")
return(data)
}
calc_regret = function(muStar, armChosen, mus) {
return(apply(as.matrix(armChosen), 2, function(row, muStar) {
return(muStar - mus[armChosen])
}, muStar))
}
cumRegret = function(regrets) {
cumRegrets = as.numeric()
for (i in 1:length(regrets)) {
cumRegrets[i] = sum(regrets[1:i])
}
return(cumRegrets)
}
analyze = function(k, sigma2, regretsEps1, regretsEps2, regretsEps3, regretsUCB, regretsSM1, regretsSM10, avgBestArmEps1, avgBestArmEps2, avgBestArmEps3, avgBestArmUCB, avgBestArmSM1, avgBestArmSM10) {
xSeq = seq(1, length(regretsEps1),1)
plot(x=xSeq, y = Convergence_Means(regretsEps1), type = "l", lwd = 2, xlab = "Iteraciones", ylab = "Arrepentimiento promedio", col = "blue", ylim = c(0,max(Convergence_Means(regretsEps1), Convergence_Means(regretsUCB))),main = paste("K = ",k, ", ", expression(sigma), " = ", sigma2))
lines(Convergence_Means(regretsUCB), lwd = 2, col = "red")
lines(Convergence_Means(regretsEps2), lwd = 2, col = "green")
lines(Convergence_Means(regretsEps3), lwd = 2, col = "black")
lines(Convergence_Means(regretsSM1), lwd = 2, col = "purple")
lines(Convergence_Means(regretsSM10), lwd = 2, col = "yellow")
cumulativeRegretEpsilon1 = cumRegret(regretsEps1)
cumulativeRegretEpsilon2 = cumRegret(regretsEps2)
cumulativeRegretEpsilon3 = cumRegret(regretsEps3)
cumulativeRegretUCV = cumRegret(regretsUCB)
cumulativeRegretSM1 = cumRegret(regretsSM1)
cumulativeRegretSM10 = cumRegret(regretsSM10)
legend("top",
col = c("blue", "red", "green", "black", "purple", "yellow"),
lwd = c(2,2),
legend = c(expression(paste(epsilon,"-greedy, ", epsilon, " = 0.5")),
"UCB",
expression(paste(epsilon," = 1/",sqrt(t))),
expression(paste(epsilon, "= 1/log(t)")),
expression(paste("Softmax, ",eta," = 0.9")),
expression(paste("Softmax, ",eta," = 10/t"))
))
plot(x=xSeq, y = cumulativeRegretEpsilon1, type = "l", lwd = 2, xlab = "Iteraciones", ylab = "Arrepentimiento cumulativo", col = "blue", ylim = c(min(cumulativeRegretUCV, cumulativeRegretEpsilon1),max(cumulativeRegretUCV, cumulativeRegretEpsilon1)),main = paste("K = ",k, ", ", expression(sigma), " = ", sigma2))
lines(cumulativeRegretUCV, lwd = 2, col = "red")
lines(cumulativeRegretEpsilon2, lwd = 2, col = "green")
lines(cumulativeRegretEpsilon3, lwd = 2, col = "black")
lines(cumulativeRegretSM1, lwd = 2, col = "purple")
lines(cumulativeRegretSM10, lwd = 2, col = "yellow")
legend("top",
col = c("blue", "red", "green", "black", "purple", "yellow"),
lwd = c(2,2),
legend = c(expression(paste(epsilon,"-greedy, ", epsilon, " = 0.5")),
"UCB",
expression(paste(epsilon," = 1/",sqrt(t))),
expression(paste(epsilon, "= 1/log(t)")),
expression(paste("Softmax, ",eta," = 0.9")),
expression(paste("Softmax, ",eta," = 10/t"))
))
## Plot best arm
plot(x=xSeq, y = avgBestArmEps1, type = "l", lwd = 2, xlab = "Iteraciones", ylab = "Veces mejor brazo elegido", col = "blue", ylim = c(0,max(avgBestArmEps1, avgBestArmEps2, avgBestArmEps3, avgBestArmUCB)),main = paste("K = ",k, ", ", expression(sigma), " = ", sigma2))
lines(avgBestArmUCB, lwd = 2, col = "red")
lines(avgBestArmEps2, lwd = 2, col = "green")
lines(avgBestArmEps3, lwd = 2, col = "black")
lines(avgBestArmSM1, lwd = 2, col = "purple")
lines(avgBestArmSM10, lwd = 2, col = "yellow")
legend("top",
col = c("blue", "red", "green", "black", "purple", "yellow"),
lwd = c(2,2),
legend = c(expression(paste(epsilon,"-greedy, ", epsilon, " = 0.5")),
"UCB",
expression(paste(epsilon," = 1/",sqrt(t))),
expression(paste(epsilon, "= 1/log(t)")),
expression(paste("Softmax, ",eta," = 0.9")),
expression(paste("Softmax, ",eta," = 10/t"))
))
}
bestArmPlayed = function(armPlayed, mus) {
percArmPlayed = as.numeric()
for (i in 1:length(armPlayed)) {
tmp = armPlayed[1:i]
percArmPlayed[i] = sum(tmp == which.max(mus))
}
return(percArmPlayed)
}
####################################
######## Initialize problem ########
####################################
set.seed(123)
# Set parameters
K = c(5,10,25,50)
sigma2 = c(0.01^2,0.1^2,1^2, 10^2)
## Set algorithm specifications
# Set epsilon mode. Sets which way the epsilon is decided. 1 for fixed, 2 for epsilon = 1/t, 3 for epsilon = 1/sqrt(t), 4 for epsilon = 1/log(t)
epsilonMode = 1
c = 1
#Setting number of max iterations and convergence criteria. Problem will run until convergence or for the max nr iterations specified.
T = 2000
nRuns = 1
convergence = Get_Convergence(mus)
# Initialize result vectors
# Generate results
for (k in K) {
for (sig in sigma2) {
for (i in 1:nRuns) {
# Get results for the run in terms of ArmChosen and reward
mus = runif(k, min = 0, max = 1)
epsilonMode = 1
epsilonResults1 = epsilonGreedy(k, mus, sig)
epsilonMode = 2
epsilonResults2 = epsilonGreedy(k, mus, sig)
epsilonMode = 3
epsilonResults3 = epsilonGreedy(k, mus, sig)
UCBResults = UCB(k, mus, sig)
if (sig != 10^2) {
softMaxC1 = SoftMax(k, mus, sig)
softMaxC10 = SoftMax(k, mus, sig, c = 10)
}
# Now, convert to AVERAGE regret over the 1000 iterations.
if (i == 1) {
avgRegretEps1 = calc_regret(mus[which.max(mus)], epsilonResults1$ArmChosen, mus)
avgRegretEps2 = calc_regret(mus[which.max(mus)], epsilonResults2$ArmChosen, mus)
avgRegretEps3 = calc_regret(mus[which.max(mus)], epsilonResults3$ArmChosen, mus)
avgRegretUCB = calc_regret(mus[which.max(mus)], UCBResults$ArmChosen, mus)
avgRegretSM1 = calc_regret(mus[which.max(mus)], softMaxC1$ArmChosen, mus)
avgRegretSM10 = calc_regret(mus[which.max(mus)], softMaxC10$ArmChosen, mus)
avgBestArmEps1 = bestArmPlayed(epsilonResults1$ArmChosen, mus)
avgBestArmEps2 = bestArmPlayed(epsilonResults2$ArmChosen, mus)
avgBestArmEps3 = bestArmPlayed(epsilonResults3$ArmChosen, mus)
avgBestArmUCB = bestArmPlayed(UCBResults$ArmChosen, mus)
avgBestArmSM1 = bestArmPlayed(softMaxC1$ArmChosen, mus)
avgBestArmSM10 = bestArmPlayed(softMaxC10$ArmChosen, mus)
} else {
avgRegretEps1 = avgRegretEps1*(i-1)/i + calc_regret(mus[which.max(mus)], epsilonResults1$ArmChosen, mus)/i
avgRegretEps2 = avgRegretEps2*(i-1)/i + calc_regret(mus[which.max(mus)], epsilonResults2$ArmChosen, mus)/i
avgRegretEps3 = avgRegretEps3*(i-1)/i + calc_regret(mus[which.max(mus)], epsilonResults3$ArmChosen, mus)/i
avgRegretUCB = avgRegretUCB*(i-1)/i + calc_regret(mus[which.max(mus)], UCBResults$ArmChosen, mus)/i
avgRegretSM1 = avgRegretSM1*(i-1)/i + calc_regret(mus[which.max(mus)], softMaxC1$ArmChosen, mus)/i
avgRegretSM10 = avgRegretSM10*(i-1)/i + calc_regret(mus[which.max(mus)], softMaxC10$ArmChosen, mus)/i
avgBestArmEps1 = avgBestArmEps1*(i-1)/i + bestArmPlayed(epsilonResults1$ArmChosen,mus)/i
avgBestArmEps2 = avgBestArmEps2*(i-1)/i + bestArmPlayed(epsilonResults2$ArmChosen,mus)/i
avgBestArmEps3 = avgBestArmEps3*(i-1)/i + bestArmPlayed(epsilonResults3$ArmChosen,mus)/i
avgBestArmUCB = avgBestArmUCB*(i-1)/i + bestArmPlayed(UCBResults$ArmChosen,mus)/i
avgBestArmSM1 = avgBestArmSM1*(i-1)/i + bestArmPlayed(softMaxC1$ArmChosen,mus)/i
avgBestArmSM10 = avgBestArmSM10*(i-1)/i + bestArmPlayed(softMaxC10$ArmChosen,mus)/i
}
print(i)
}
analyze(k, sqrt(sig), avgRegretEps1, avgRegretEps2, avgRegretEps3, avgRegretUCB, avgRegretSM1, avgRegretSM10, avgBestArmEps1, avgBestArmEps2, avgBestArmEps3, avgBestArmUCB, avgBestArmSM1, avgBestArmSM10)
}
}
data = read.csv("australian-crabs.csv")
# x1 = matrix(c(4,2,2,4,2,3,3,6,4,4), nrow = 5, ncol = 2, byrow = TRUE)
#
# x2 = matrix(c(9,10,6,8,9,5,8,7,10,8), nrow = 5, ncol = 2, byrow = TRUE)
#
# # Nice tutorial http://www.sci.utah.edu/~shireen/pdfs/tutorials/Elhabian_LDA09.pdf
#
# s1 = cov(x1)
# s2 = cov(x2)
# s_w = s1 + s2
#
# # Take the mean of each dimension in each set, i.e. for each class.
# mu_1 = c(mean(x1[,1]), mean(x1[,2]))
#
# mu_2 = c(mean(x2[,1]), mean(x2[,2]))
#
# s_B = as.matrix(mu_1 - mu_2)%*%t(as.matrix(mu_1 - mu_2))
#
# interestingMatrix = solve(s_w)%*%s_B
#
# eigenVals = eigen(interestingMatrix)
#w_star = solve(s_w)%*%as.matrix(mu_1 - mu_2)
DiscFunc = function(x, cov_k, mu_k, prior) {
x = as.matrix(x)
mu_k = as.matrix(mu_k)
#print(dim(x))
#print(dim(solve(cov_k)))
express = t(x)%*%solve(cov_k)%*%mu_k - (1/2)%*%t(mu_k)%*%solve(cov_k)%*%mu_k + log(prior)
return(express)
}
# This function assumes only two dimensions of the data! Or does it?
# Send in data as features as columns, data points as rows
binaryLDA = function(x1, x2, priors, test, classifications = c("2","1")) {
mu_1 = apply(x1, 2, mean)
mu_2 = apply(x2, 2, mean)
cov_1 = cov(x1)
cov_2 = cov(x2)
n1 = nrow(x1)
n2 = nrow(x2)
N = n1+n2
covM = 1/N*(n1*cov_1 + n2*cov_2)
classificationsTest = apply(test, 1, function(row) {
class1 = DiscFunc(row, covM, mu_1, priors[1])
class2 = DiscFunc(row, covM, mu_2, priors[2])
return(which.max(c(class1,class2)))
})
classificationsTest = factor(ifelse(classificationsTest == 2, classifications[2], classifications[1]))
# Is proportional to, so we do not get the constant needed.
decisionBoundary = solve(covM)%*%(mu_2 - mu_1)
return(classificationsTest, decisionBoundary)
}
x2 = matrix(c(data[data$sex == "Male",]$CL, data[data$sex == "Male",]$RW), nrow = nrow(data[data$sex == "Male",]), ncol = 2)
x1 = matrix(c(data[data$sex == "Female",]$CL, data[data$sex == "Female",]$RW), nrow = nrow(data[data$sex == "Female",]), ncol = 2)
test_m = matrix(c(data$CL, data$RW), nrow = nrow(data), ncol = 2)
data = read.csv("australian-crabs.csv")
setwd("~/Desktop/Plugg_Lkpg/HT18/TDDE01/Lab 2")
data = read.csv("australian-crabs.csv")
DiscFunc = function(x, cov_k, mu_k, prior) {
x = as.matrix(x)
mu_k = as.matrix(mu_k)
#print(dim(x))
#print(dim(solve(cov_k)))
express = t(x)%*%solve(cov_k)%*%mu_k - (1/2)%*%t(mu_k)%*%solve(cov_k)%*%mu_k + log(prior)
return(express)
}
# This function assumes only two dimensions of the data! Or does it?
# Send in data as features as columns, data points as rows
binaryLDA = function(x1, x2, priors, test, classifications = c("2","1")) {
mu_1 = apply(x1, 2, mean)
mu_2 = apply(x2, 2, mean)
cov_1 = cov(x1)
cov_2 = cov(x2)
n1 = nrow(x1)
n2 = nrow(x2)
N = n1+n2
covM = 1/N*(n1*cov_1 + n2*cov_2)
classificationsTest = apply(test, 1, function(row) {
class1 = DiscFunc(row, covM, mu_1, priors[1])
class2 = DiscFunc(row, covM, mu_2, priors[2])
return(which.max(c(class1,class2)))
})
classificationsTest = factor(ifelse(classificationsTest == 2, classifications[2], classifications[1]))
# Is proportional to, so we do not get the constant needed.
decisionBoundary = solve(covM)%*%(mu_2 - mu_1)
return(classificationsTest, decisionBoundary)
}
x2 = matrix(c(data[data$sex == "Male",]$CL, data[data$sex == "Male",]$RW), nrow = nrow(data[data$sex == "Male",]), ncol = 2)
x1 = matrix(c(data[data$sex == "Female",]$CL, data[data$sex == "Female",]$RW), nrow = nrow(data[data$sex == "Female",]), ncol = 2)
test_m = matrix(c(data$CL, data$RW), nrow = nrow(data), ncol = 2)
# This function assumes only two dimensions of the data! Or does it?
# Send in data as features as columns, data points as rows
binaryLDA = function(x1, x2, priors, test, classifications = c("2","1")) {
mu_1 = apply(x1, 2, mean)
mu_2 = apply(x2, 2, mean)
cov_1 = cov(x1)
cov_2 = cov(x2)
n1 = nrow(x1)
n2 = nrow(x2)
N = n1+n2
covM = 1/N*(n1*cov_1 + n2*cov_2)
classificationsTest = apply(test, 1, function(row) {
class1 = DiscFunc(row, covM, mu_1, priors[1])
class2 = DiscFunc(row, covM, mu_2, priors[2])
return(which.max(c(class1,class2)))
})
classificationsTest = factor(ifelse(classificationsTest == 2, classifications[2], classifications[1]))
# Is proportional to, so we do not get the constant needed.
decisionBoundary = solve(covM)%*%(mu_2 - mu_1)
return(classificationsTest)
}
classificationsTest = binaryLDA(x1, x2, priors = c(0.5,0.5), classifications = c("Male", "Female"))
classificationsTest = binaryLDA(x1, x2, priors = c(0.5,0.5),test = test_m, classifications = c("Male", "Female"))
table(data$sex, classificationsTest)
table(data$sex, classificationsTest)
classificationsTest = binaryLDA(x1, x2, priors = c(0.5,0.5),test = test_m, classifications = c("Female", "Male"))
table(data$sex, classificationsTest)
setwd("~/Desktop/Programmeringsprojekt/ML-Implementations/Filips ML package")
#https://hilaryparker.com/2014/04/29/writing-an-r-package-from-scratch/
# create("Filips.ML.package")
# setwd("./cats")
# document()
#' When update to package is done (for Filips ML package):
#' 1. Set working directory to package.
#' 2. setwd("~/Desktop/Programmeringsprojekt/Machine Learning/Filips ML package/Filips.ML.package")
#'    document()
#' 3. Set working directory to folder above and reinstall the package with updates.
#'    setwd("~/Desktop/Programmeringsprojekt/Machine Learning/Filips ML package")
#'    install("Filips.ML.package")
#'
# To use the ML functions, run the code below.
library("devtools")
library(roxygen2)
#setwd("~/Desktop/Programmeringsprojekt/Machine Learning/Code/Filips ML package/Filips.ML.package")
document()
#setwd("~/Desktop/Programmeringsprojekt/Machine Learning/Code/Filips ML package")
install("Filips.ML.package")
#this.dir <- dirname(parent.frame(2)$ofile) #To set it to source file location
#setwd(this.dir) #To set it to source file location
help("Filips.ML.package")
#this.dir <- dirname(parent.frame(2)$ofile) #To set it to source file location
#setwd(this.dir) #To set it to source file location
library("Filips.ML.package")
#https://hilaryparker.com/2014/04/29/writing-an-r-package-from-scratch/
# create("Filips.ML.package")
# setwd("./cats")
# document()
#' When update to package is done (for Filips ML package):
#' 1. Set working directory to package.
#' 2. setwd("~/Desktop/Programmeringsprojekt/Machine Learning/Filips ML package/Filips.ML.package")
#'    document()
#' 3. Set working directory to folder above and reinstall the package with updates.
#'    setwd("~/Desktop/Programmeringsprojekt/Machine Learning/Filips ML package")
#'    install("Filips.ML.package")
#'
# To use the ML functions, run the code below.
library("devtools")
library(roxygen2)
#setwd("~/Desktop/Programmeringsprojekt/Machine Learning/Code/Filips ML package/Filips.ML.package")
document()
#setwd("~/Desktop/Programmeringsprojekt/Machine Learning/Code/Filips ML package")
install("Filips.ML.package")
#this.dir <- dirname(parent.frame(2)$ofile) #To set it to source file location
#setwd(this.dir) #To set it to source file location
library("Filips.ML.package")
detach("package:Filips.ML.package", unload=TRUE)
library("Filips.ML.package", lib.loc="/Library/Frameworks/R.framework/Versions/3.4/Resources/library")
#https://hilaryparker.com/2014/04/29/writing-an-r-package-from-scratch/
# create("Filips.ML.package")
# setwd("./cats")
# document()
#' When update to package is done (for Filips ML package):
#' 1. Set working directory to package.
#' 2. setwd("~/Desktop/Programmeringsprojekt/Machine Learning/Filips ML package/Filips.ML.package")
#'    document()
#' 3. Set working directory to folder above and reinstall the package with updates.
#'    setwd("~/Desktop/Programmeringsprojekt/Machine Learning/Filips ML package")
#'    install("Filips.ML.package")
#'
# To use the ML functions, run the code below.
library("devtools")
library(roxygen2)
setwd("~/Desktop/Programmeringsprojekt/ML-Implementations/Filips ML package")
document()
setwd("~/Desktop/Programmeringsprojekt/Machine Learning/Code/Filips ML package/Filips.ML.package")
install("Filips.ML.package")
#this.dir <- dirname(parent.frame(2)$ofile) #To set it to source file location
#setwd(this.dir) #To set it to source file location
library("Filips.ML.package")
setwd("~/Desktop/Programmeringsprojekt/Machine Learning/Code/Filips ML package/Filips.ML.package")
setwd("~/Desktop/Programmeringsprojekt/ML-Implementations/Filips ML package")
document()
library("devtools")
library(roxygen2)
document()
setwd("~/Desktop/Programmeringsprojekt/ML-Implementations/Filips ML package")
document()
setwd("~/Desktop/Programmeringsprojekt/ML-Implementations/Filips ML package")
document()
setwd("~/Desktop/Programmeringsprojekt/Machine Learning/Code/Filips ML package/Filips.ML.package")
document()
setwd("~/Desktop/Programmeringsprojekt/ML-Implementations/Filips ML package")
document()
setwd("~/Desktop/Programmeringsprojekt/Machine Learning/Code/Filips ML package/Filips.ML.package")
setwd("~/Desktop/Programmeringsprojekt/Machine Learning/Code/Filips ML package/Filips.ML.package")
detach("package:Filips.ML.package", unload=TRUE)
setwd("~/Desktop/Programmeringsprojekt/Machine Learning/Code/Filips ML package/Filips.ML.package")
setwd("~/Desktop/Programmeringsprojekt/ML-Implementations/Filips ML package")
setwd("/Users/filipcornell/Desktop/Programmeringsprojekt/ML-Implementations/Filips ML package/Filips.ML.package")
setwd("~/Desktop/Programmeringsprojekt/ML-Implementations/Filips ML package")
document()
setwd("/Users/filipcornell/Desktop/Programmeringsprojekt/ML-Implementations/Filips ML package/Filips.ML.package")
document()
setwd("~/Desktop/Programmeringsprojekt/ML-Implementations/Filips ML package")
install("Filips.ML.package")
#this.dir <- dirname(parent.frame(2)$ofile) #To set it to source file location
#setwd(this.dir) #To set it to source file location
library("Filips.ML.package")
setwd("/Users/filipcornell/Desktop/Programmeringsprojekt/ML-Implementations/Filips ML package/Filips.ML.package")
document()
setwd("/Users/filipcornell/Desktop/Programmeringsprojekt/ML-Implementations/Filips ML package")
install("Filips.ML.package")
help(ADAM)
help("binaryLDA")
