muEstimates = normalize(muEstimates)
print(muEstimates)
probs = exp(eta_t*muEstimates)/sum(exp(eta_t*muEstimates))
print(probs)
}
armChosen[t] = sample(seq(1,k,1), size = 1, prob = probs)
rewards[t] = rnorm(n=1,mean = mus[armChosen[t]], sd = sqrt(sigma2))
n_k_t[armChosen[t]] = n_k_t[armChosen[t]] + 1
muEstimates[armChosen[t]] = sum(rewards[armChosen == armChosen[t]])/(n_k_t[armChosen[t]] + max(0, 1 - n_k_t[armChosen[t]]))
}
data = data.frame(armChosen, rewards)
colnames(data) = c("ArmChosen", "Rewards")
return(data)
}
UCB = function(k, mus, sigma2) {
muEstimates = rep(0.5, k)
# Initialize vectors to store information
rewards = as.numeric()
armChosen = as.numeric()
n_k_t = rep(0,k)
t = 1
while (t <= T) {
# Generate the ucbs. We need the indices for this, hence the for loop.
ucbs = as.numeric()
for (i in 1:k) {
ucbs[i] = muEstimates[i] + sqrt(2*log(t)/(n_k_t[i] + max(0, 1 - n_k_t[i])))
}
# Choose arm from the one with highest UCB
armChosen[t] = which.max(ucbs)
# update n_k_t
n_k_t[armChosen[t]] = n_k_t[armChosen[t]] + 1
# Sample reward
rewards[t] = rnorm(n=1, mean = mus[armChosen[t]], sd = sqrt(sigma2))
# Update muEstimates
muEstimates[armChosen[t]] = sum(rewards[armChosen == armChosen[t]])/(n_k_t[armChosen[t]] + max(0, 1 - n_k_t[armChosen[t]]))
t = t + 1
}
data = data.frame(armChosen, rewards)
colnames(data) = c("ArmChosen", "Rewards")
return(data)
}
calc_regret = function(muStar, armChosen, mus) {
return(apply(as.matrix(armChosen), 2, function(row, muStar) {
return(muStar - mus[armChosen])
}, muStar))
}
cumRegret = function(regrets) {
cumRegrets = as.numeric()
for (i in 1:length(regrets)) {
cumRegrets[i] = sum(regrets[1:i])
}
return(cumRegrets)
}
analyze = function(k, sigma2, regretsEps1, regretsEps2, regretsEps3, regretsUCB, regretsSM1, regretsSM10, avgBestArmEps1, avgBestArmEps2, avgBestArmEps3, avgBestArmUCB, avgBestArmSM1, avgBestArmSM10) {
xSeq = seq(1, length(regretsEps1),1)
plot(x=xSeq, y = Convergence_Means(regretsEps1), type = "l", lwd = 2, xlab = "Iteraciones", ylab = "Arrepentimiento promedio", col = "blue", ylim = c(0,max(Convergence_Means(regretsEps1), Convergence_Means(regretsUCB))),main = paste("K = ",k, ", ", expression(sigma), " = ", sigma2))
lines(Convergence_Means(regretsUCB), lwd = 2, col = "red")
lines(Convergence_Means(regretsEps2), lwd = 2, col = "green")
lines(Convergence_Means(regretsEps3), lwd = 2, col = "black")
lines(Convergence_Means(regretsSM1), lwd = 2, col = "purple")
lines(Convergence_Means(regretsSM10), lwd = 2, col = "yellow")
cumulativeRegretEpsilon1 = cumRegret(regretsEps1)
cumulativeRegretEpsilon2 = cumRegret(regretsEps2)
cumulativeRegretEpsilon3 = cumRegret(regretsEps3)
cumulativeRegretUCV = cumRegret(regretsUCB)
cumulativeRegretSM1 = cumRegret(regretsSM1)
cumulativeRegretSM10 = cumRegret(regretsSM10)
legend("top",
col = c("blue", "red", "green", "black", "purple", "yellow"),
lwd = c(2,2),
legend = c(expression(paste(epsilon,"-greedy, ", epsilon, " = 0.5")),
"UCB",
expression(paste(epsilon," = 1/",sqrt(t))),
expression(paste(epsilon, "= 1/log(t)")),
expression(paste("Softmax, ",eta," = 0.9")),
expression(paste("Softmax, ",eta," = 10/t"))
))
plot(x=xSeq, y = cumulativeRegretEpsilon1, type = "l", lwd = 2, xlab = "Iteraciones", ylab = "Arrepentimiento cumulativo", col = "blue", ylim = c(min(cumulativeRegretUCV, cumulativeRegretEpsilon1),max(cumulativeRegretUCV, cumulativeRegretEpsilon1)),main = paste("K = ",k, ", ", expression(sigma), " = ", sigma2))
lines(cumulativeRegretUCV, lwd = 2, col = "red")
lines(cumulativeRegretEpsilon2, lwd = 2, col = "green")
lines(cumulativeRegretEpsilon3, lwd = 2, col = "black")
lines(cumulativeRegretSM1, lwd = 2, col = "purple")
lines(cumulativeRegretSM10, lwd = 2, col = "yellow")
legend("top",
col = c("blue", "red", "green", "black", "purple", "yellow"),
lwd = c(2,2),
legend = c(expression(paste(epsilon,"-greedy, ", epsilon, " = 0.5")),
"UCB",
expression(paste(epsilon," = 1/",sqrt(t))),
expression(paste(epsilon, "= 1/log(t)")),
expression(paste("Softmax, ",eta," = 0.9")),
expression(paste("Softmax, ",eta," = 10/t"))
))
## Plot best arm
plot(x=xSeq, y = avgBestArmEps1, type = "l", lwd = 2, xlab = "Iteraciones", ylab = "Veces mejor brazo elegido", col = "blue", ylim = c(0,max(avgBestArmEps1, avgBestArmEps2, avgBestArmEps3, avgBestArmUCB)),main = paste("K = ",k, ", ", expression(sigma), " = ", sigma2))
lines(avgBestArmUCB, lwd = 2, col = "red")
lines(avgBestArmEps2, lwd = 2, col = "green")
lines(avgBestArmEps3, lwd = 2, col = "black")
lines(avgBestArmSM1, lwd = 2, col = "purple")
lines(avgBestArmSM10, lwd = 2, col = "yellow")
legend("top",
col = c("blue", "red", "green", "black", "purple", "yellow"),
lwd = c(2,2),
legend = c(expression(paste(epsilon,"-greedy, ", epsilon, " = 0.5")),
"UCB",
expression(paste(epsilon," = 1/",sqrt(t))),
expression(paste(epsilon, "= 1/log(t)")),
expression(paste("Softmax, ",eta," = 0.9")),
expression(paste("Softmax, ",eta," = 10/t"))
))
}
bestArmPlayed = function(armPlayed, mus) {
percArmPlayed = as.numeric()
for (i in 1:length(armPlayed)) {
tmp = armPlayed[1:i]
percArmPlayed[i] = sum(tmp == which.max(mus))
}
return(percArmPlayed)
}
####################################
######## Initialize problem ########
####################################
set.seed(123)
# Set parameters
K = c(5,10,25,50)
sigma2 = c(0.01^2,0.1^2,1^2, 10^2)
## Set algorithm specifications
# Set epsilon mode. Sets which way the epsilon is decided. 1 for fixed, 2 for epsilon = 1/t, 3 for epsilon = 1/sqrt(t), 4 for epsilon = 1/log(t)
epsilonMode = 1
c = 1
#Setting number of max iterations and convergence criteria. Problem will run until convergence or for the max nr iterations specified.
T = 2000
nRuns = 1
convergence = Get_Convergence(mus)
# Initialize result vectors
# Generate results
for (k in K) {
for (sig in sigma2) {
for (i in 1:nRuns) {
# Get results for the run in terms of ArmChosen and reward
mus = runif(k, min = 0, max = 1)
epsilonMode = 1
epsilonResults1 = epsilonGreedy(k, mus, sig)
epsilonMode = 2
epsilonResults2 = epsilonGreedy(k, mus, sig)
epsilonMode = 3
epsilonResults3 = epsilonGreedy(k, mus, sig)
UCBResults = UCB(k, mus, sig)
if (sig != 10^2) {
softMaxC1 = SoftMax(k, mus, sig)
softMaxC10 = SoftMax(k, mus, sig, c = 10)
}
# Now, convert to AVERAGE regret over the 1000 iterations.
if (i == 1) {
avgRegretEps1 = calc_regret(mus[which.max(mus)], epsilonResults1$ArmChosen, mus)
avgRegretEps2 = calc_regret(mus[which.max(mus)], epsilonResults2$ArmChosen, mus)
avgRegretEps3 = calc_regret(mus[which.max(mus)], epsilonResults3$ArmChosen, mus)
avgRegretUCB = calc_regret(mus[which.max(mus)], UCBResults$ArmChosen, mus)
avgRegretSM1 = calc_regret(mus[which.max(mus)], softMaxC1$ArmChosen, mus)
avgRegretSM10 = calc_regret(mus[which.max(mus)], softMaxC10$ArmChosen, mus)
avgBestArmEps1 = bestArmPlayed(epsilonResults1$ArmChosen, mus)
avgBestArmEps2 = bestArmPlayed(epsilonResults2$ArmChosen, mus)
avgBestArmEps3 = bestArmPlayed(epsilonResults3$ArmChosen, mus)
avgBestArmUCB = bestArmPlayed(UCBResults$ArmChosen, mus)
avgBestArmSM1 = bestArmPlayed(softMaxC1$ArmChosen, mus)
avgBestArmSM10 = bestArmPlayed(softMaxC10$ArmChosen, mus)
} else {
avgRegretEps1 = avgRegretEps1*(i-1)/i + calc_regret(mus[which.max(mus)], epsilonResults1$ArmChosen, mus)/i
avgRegretEps2 = avgRegretEps2*(i-1)/i + calc_regret(mus[which.max(mus)], epsilonResults2$ArmChosen, mus)/i
avgRegretEps3 = avgRegretEps3*(i-1)/i + calc_regret(mus[which.max(mus)], epsilonResults3$ArmChosen, mus)/i
avgRegretUCB = avgRegretUCB*(i-1)/i + calc_regret(mus[which.max(mus)], UCBResults$ArmChosen, mus)/i
avgRegretSM1 = avgRegretSM1*(i-1)/i + calc_regret(mus[which.max(mus)], softMaxC1$ArmChosen, mus)/i
avgRegretSM10 = avgRegretSM10*(i-1)/i + calc_regret(mus[which.max(mus)], softMaxC10$ArmChosen, mus)/i
avgBestArmEps1 = avgBestArmEps1*(i-1)/i + bestArmPlayed(epsilonResults1$ArmChosen,mus)/i
avgBestArmEps2 = avgBestArmEps2*(i-1)/i + bestArmPlayed(epsilonResults2$ArmChosen,mus)/i
avgBestArmEps3 = avgBestArmEps3*(i-1)/i + bestArmPlayed(epsilonResults3$ArmChosen,mus)/i
avgBestArmUCB = avgBestArmUCB*(i-1)/i + bestArmPlayed(UCBResults$ArmChosen,mus)/i
avgBestArmSM1 = avgBestArmSM1*(i-1)/i + bestArmPlayed(softMaxC1$ArmChosen,mus)/i
avgBestArmSM10 = avgBestArmSM10*(i-1)/i + bestArmPlayed(softMaxC10$ArmChosen,mus)/i
}
print(i)
}
analyze(k, sqrt(sig), avgRegretEps1, avgRegretEps2, avgRegretEps3, avgRegretUCB, avgRegretSM1, avgRegretSM10, avgBestArmEps1, avgBestArmEps2, avgBestArmEps3, avgBestArmUCB, avgBestArmSM1, avgBestArmSM10)
}
}
data = read.csv("australian-crabs.csv")
DiscFunc = function(x, cov_k, mu_k, prior) {
x = as.matrix(x)
mu_k = as.matrix(mu_k)
express = t(x)%*%solve(cov_k)%*%mu_k -
(1/2)%*%t(mu_k)%*%solve(cov_k)%*%mu_k + log(prior)
return(express)
}
classificationRate = function(confMatrix) {
return(sum(diag(confMatrix))/sum(confMatrix))
}
# This function assumes only two dimensions of the data!
# Send in data as features as columns, data points as rows
LDA = function(x1, x2, prior1, prior2, test) {
mu_1 = apply(x1, 2, mean)
mu_2 = apply(x2, 2, mean)
cov_1 = cov(x1)
cov_2 = cov(x2)
n1 = nrow(x1)
n2 = nrow(x2)
N = n1+n2
covM = 1/N*(n1*cov_1 + n2*cov_2)
classificationsTest = apply(test, 1, function(row) {
class1 = DiscFunc(row, covM, mu_1, prior1)
class2 = DiscFunc(row, covM, mu_2, prior2)
return(which.max(c(class1,class2)))
})
classificationsTest = factor(ifelse(classificationsTest == 2, "Male", "Female"))
# Is proportional to, so we do not get the constant needed.
decisionBoundary = solve(covM)%*%(mu_2 - mu_1)
return(list(classificationsTest, decisionBoundary))
}
x2 = matrix(c(data[data$sex == "Male",]$CL,
data[data$sex == "Male",]$RW),
nrow = nrow(data[data$sex == "Male",]), ncol = 2)
x1 = matrix(c(data[data$sex == "Female",]$CL,
data[data$sex == "Female",]$RW),
nrow = nrow(data[data$sex == "Female",]), ncol = 2)
test_m = matrix(c(data$CL, data$RW), nrow = nrow(data), ncol = 2)
result = LDA(x1,x2,prior1 = 0.5,prior2 = 0.5, test = test_m)
classificationsTest = result[[1]]
decisionB = result[[2]]
print("Misclassification rate test")
table(data$sex, classificationsTest)
1 - classificationRate(table(data$sex, classificationsTest))
library(MASS)
fitModel = lda(formula = sex ~ RW + CL,
data = data,
prior = c(length(data$sex[data$sex == "Male"])
/nrow(data),length(data$sex[data$sex == "Female"])
/nrow(data)))
fits = predict(fitModel, data)
confMatrix = table(data$sex, fits$class)
confMatrix
print("misclassificionation rate test")
1 - classificationRate(confMatrix)
plot(data[data$sex == "Male",]$CL,
data[data$sex == "Male",]$RW,
col = "green", xlab = "CL", ylab = "RW")
points(data[data$sex == "Female",]$CL,
data[data$sex == "Female",]$RW, col = "red")
xSeq = seq(min(data$CL), max(data$CL))
c = 1.7 # How find constant c?
RWvals = -(decisionB[1,]*xSeq/decisionB[2,]) + c
lines(x=xSeq, y = RWvals)
## Special task 4
classificationRate = function(confMatrix) {
return(sum(diag(confMatrix))/sum(confMatrix))
}
data = read.csv("australian-crabs.csv")
n=dim(data)[1]
set.seed(12345)
id=sample(1:n, floor(n*0.5))
train=data[id,]
test=data[-id,]
NaiveBayesImpl = function(train, test) {
# Divide into subsets of the classes
trainBlue = train[train$species == "Blue",]
trainOrange = train[train$species == "Orange",]
# Set priors proportional to the class sizes
priorBlue = nrow(trainBlue)/(nrow(trainBlue) + nrow(trainOrange))
priorOrange = 1 - priorBlue
# Estimate the densities for the training sets
# through density() function for each class for each measurement variable.
# Could definitely be done smoother than this, but this works too.
BlueCLDens = density(trainBlue$CL)
OrangeCLDens = density(trainOrange$CL)
BlueRWDens = density(trainBlue$RW)
OrangeRWDens = density(trainOrange$RW)
BlueFLDens = density(trainBlue$FL)
OrangeFLDens = density(trainOrange$FL)
BlueCWDens = density(trainBlue$CW)
OrangeCWDens = density(trainOrange$CW)
BlueBDDens = density(trainBlue$BD)
OrangeBDDens = density(trainOrange$BD)
trainFeats = as.matrix(train[,-c(1,2,3)])
testFeats = as.matrix(test[,-c(1,2,3)])
# Classify them according to unnormalized probabilities
#obtained through densities calculated above.
# This assumes conditional
#independence of the variables, just like Naive Bayes
trainClassifications = factor(apply(trainFeats, 1, function(row) {
blue = approx(BlueFLDens$x, BlueFLDens$y, xout = row[1])$y*
approx(BlueRWDens$x, BlueRWDens$y, xout = row[2])$y*
approx(BlueCLDens$x,BlueCLDens$y,xout=row[3])$y*
approx(BlueCWDens$x, BlueCWDens$y, xout = row[4])$y*
approx(BlueBDDens$x, BlueBDDens$y, xout = row[5])$y*
priorBlue
orange = approx(OrangeFLDens$x, OrangeFLDens$y, xout = row[1])$y*
approx(OrangeRWDens$x, OrangeRWDens$y, xout = row[2])$y*
approx(OrangeCLDens$x,OrangeCLDens$y,xout=row[3])$y*
approx(OrangeCWDens$x, OrangeCWDens$y, xout = row[4])$y*
approx(OrangeBDDens$x, OrangeBDDens$y, xout = row[5])$y*
priorOrange
return(ifelse(blue >= orange, "blue", "orange"))
}))
print("Confusion matrix train")
print(table(train$species, trainClassifications))
print("Misclassification rate train")
print(1 - classificationRate(table(train$species, trainClassifications)))
testClassifications = factor(apply(testFeats, 1, function(row) {
blue = approx(BlueFLDens$x, BlueFLDens$y, xout = row[1])$y*
approx(BlueRWDens$x, BlueRWDens$y, xout = row[2])$y*
approx(BlueCLDens$x,BlueCLDens$y,xout=row[3])$y*
approx(BlueCWDens$x, BlueCWDens$y, xout = row[4])$y*
approx(BlueBDDens$x, BlueBDDens$y, xout = row[5])$y*
priorBlue
orange = approx(OrangeFLDens$x, OrangeFLDens$y, xout = row[1])$y*
approx(OrangeRWDens$x, OrangeRWDens$y, xout = row[2])$y*
approx(OrangeCLDens$x,OrangeCLDens$y,xout=row[3])$y*
approx(OrangeCWDens$x, OrangeCWDens$y, xout = row[4])$y*
approx(OrangeBDDens$x, OrangeBDDens$y, xout = row[5])$y*
priorOrange
return(ifelse(blue >= orange, "blue", "orange"))
}))
print("Confusion matrix test")
print(table(test$species, trainClassifications))
print("Misclassification rate test")
print(1 - classificationRate(table(test$species, trainClassifications)))
}
NaiveBayesImpl(train, test)
dataBlue = data[data$species = "Blue",]
dataBlue = data[data$species == "Blue",]
dataOrange = data[data$species == "Orange",]
cov(as.matrix(data[,-c(1,2,3)]))
dataBlue = data[data$species == "Blue",-c(1,2,3)]
dataOrange = data[data$species == "Orange",-c(1,2,3)]
cov(as.matrix(data[,-c(1,2,3)]))
cov(as.matrix(dataBlue))
cov(as.matrix(dataOrange))
dataBlue = data[data$species == "Blue",-c(1,2,3)]
dataOrange = data[data$species == "Orange",-c(1,2,3)]
cov(as.matrix(data[,-c(1,2,3)]))
"Covariance blue species"
cov(as.matrix(dataBlue))
"Covariance orange species"
cov(as.matrix(dataOrange))
dataBlue = data[data$species == "Blue",-c(1,2,3)]
dataOrange = data[data$species == "Orange",-c(1,2,3)]
cov(as.matrix(data[,-c(1,2,3)]))
"Covariance blue species"
cov(as.matrix(dataBlue))
"Covariance orange species"
cov(as.matrix(dataOrange))
dataBlue = data[data$species == "Blue",-c(1,2,3)]
dataOrange = data[data$species == "Orange",-c(1,2,3)]
"Covariance all species"
cov(as.matrix(data[,-c(1,2,3)]))
"Covariance blue species"
cov(as.matrix(dataBlue))
"Covariance orange species"
cov(as.matrix(dataOrange))
setwd("~/Desktop/Plugg_Lkpg/HT18/Period 1/TDDE15/Labbar/Lab4")
SE_kernel <- function(X, xPrim, sigF, l) {
n1 = length(X)
n2 = length(xPrim)
K <- matrix(NA,n1,n2)
for (i in 1:n2) {
K[,i] <- sigF^2*exp(-0.5*( (X-xPrim[i])/l)^2 )
}
return(K)
}
#Left is only if it is one-sided
CIForIntervalForFunction <- function(mean, var, p, twoSided = TRUE, left = FALSE) {
if (twoSided == TRUE) {
upper = mean + qnorm(1 - (1 - p)/2)*sqrt(var)
lower = mean + qnorm((1 - p)/2)*sqrt(var)
return(data.frame(lower,upper))
} else {
if (left == TRUE) {
return(mean - qnorm(p)*var)
} else {
return(mean + qnorm(1 - p)*var)
}
}
}
plotP1 = function(mean, var, x, y, xSeq, title) {
CI = CIForIntervalForFunction(mean, var, 0.95)
plot(x = xSeq,
y = mean,
type = 'l',
ylab = "Density",
xlab = "x-value",
ylim = c(-2.2,2.2),
main = title)
polygon(c(xSeq, rev(xSeq)),
c(CI$lower, rev(CI$upper)),
col = "lightgrey",
border = NA)
points(x = x, y = y)
lines(x = xSeq,
y = post[[1]],
ylab = "Density",
xlab = "x-value",
ylim = c(-2.2,2.2))
}
SE_kernel <- function(X, xPrim, sigF, l) {
n1 = length(X)
n2 = length(xPrim)
K <- matrix(NA,n1,n2)
for (i in 1:n2) {
K[,i] <- sigF^2*exp(-0.5*( (X-xPrim[i])/l)^2 )
}
return(K)
}
#Left is only if it is one-sided
CIForIntervalForFunction <- function(mean, var, p, twoSided = TRUE, left = FALSE) {
if (twoSided == TRUE) {
upper = mean + qnorm(1 - (1 - p)/2)*sqrt(var)
lower = mean + qnorm((1 - p)/2)*sqrt(var)
return(data.frame(lower,upper))
} else {
if (left == TRUE) {
return(mean - qnorm(p)*var)
} else {
return(mean + qnorm(1 - p)*var)
}
}
}
plotP1 = function(mean, var, x, y, xSeq, title) {
CI = CIForIntervalForFunction(mean, var, 0.95)
plot(x = xSeq,
y = mean,
type = 'l',
ylab = "Density",
xlab = "x-value",
ylim = c(-2.2,2.2),
main = title)
polygon(c(xSeq, rev(xSeq)),
c(CI$lower, rev(CI$upper)),
col = "lightgrey",
border = NA)
points(x = x, y = y)
lines(x = xSeq,
y = post[[1]],
ylab = "Density",
xlab = "x-value",
ylim = c(-2.2,2.2))
}
#Define params
sigmaNoise = 0.1
xSeq = seq(-1,1,0.005)
l = 0.3
sigF = 1
#TASK 2
x = 0.4
y = 0.719
post = posteriorGP(X = x,
y = y,
xStar = xSeq,
hyperParam = c(sigF,l),
sigmaNoise = sigmaNoise)
#TASK 1
#Implementing algorithm from book.
posteriorGP <- function(X, y, xStar, hyperParam, sigmaNoise) {
K = SE_kernel(X = X, xPrim = X, sig = hyperParam[1], l = hyperParam[2])
L = t(chol(K + diag(sigmaNoise^2, nrow = length(X), ncol = length(X))))
L.transpose = t(L)
alpha = solve(L.transpose, solve(L,y)) #Divide by = multiply by inverse.
kStar = SE_kernel(X, xStar, sig = hyperParam[1], l = hyperParam[2])
fStarMean = t(kStar) %*% alpha
v = solve(L, kStar)
kStarStar = SE_kernel(xStar, xStar, sig = hyperParam[1], l = hyperParam[2])
varFStar = kStarStar - t(v) %*% v
return(list(fStarMean, varFStar))
}
#Define params
sigmaNoise = 0.1
xSeq = seq(-1,1,0.005)
l = 0.3
sigF = 1
#TASK 2
x = 0.4
y = 0.719
post = posteriorGP(X = x,
y = y,
xStar = xSeq,
hyperParam = c(sigF,l),
sigmaNoise = sigmaNoise)
plotP1(mean = post[[1]],
var = diag(post[[2]]),
x = x,
y = y,
xSeq = xSeq,
title = "Task 2")
#https://hilaryparker.com/2014/04/29/writing-an-r-package-from-scratch/
# create("Filips.ML.package")
# setwd("./cats")
# document()
#' When update to package is done (for Filips ML package):
#' 1. Set working directory to package.
#' 2. setwd("~/Desktop/Programmeringsprojekt/Machine Learning/Filips ML package/Filips.ML.package")
#'    document()
#' 3. Set working directory to folder above and reinstall the package with updates.
#'    setwd("~/Desktop/Programmeringsprojekt/Machine Learning/Filips ML package")
#'    install("Filips.ML.package")
#'
# To use the ML functions, run the code below.
library("devtools")
library(roxygen2)
setwd("/Users/filipcornell/Desktop/Programmeringsprojekt/ML-Implementations/Filips ML package/Filips.ML.package")
document()
setwd("/Users/filipcornell/Desktop/Programmeringsprojekt/ML-Implementations/Filips ML package")
install("Filips.ML.package")
#this.dir <- dirname(parent.frame(2)$ofile) #To set it to source file location
#setwd(this.dir) #To set it to source file location
library("Filips.ML.package")
