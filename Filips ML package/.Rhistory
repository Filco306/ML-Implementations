# Update muEstimates
muEstimates[armChosen[t]] = sum(rewards[armChosen == armChosen[t]])/sum(armChosen == armChosen[t])
# Update probabilities
probabilities = rep(epsilon/k, k)
# Set the best arm's probability to 1 - epsilon + e/k, i.e. add 1 - epsilon
probabilities[which.max(muEstimates)] = probabilities[which.max(muEstimates)] + 1 - epsilon
t = t + 1
}
data = data.frame(armChosen, rewards)
colnames(data) = c("ArmChosen", "Rewards")
return(data)
}
SoftMax = function(k, mus, sigma2, c = NULL) {
muEstimates = rep(0.5, k)
# Initialize vectors to store information
rewards = as.numeric()
armChosen = as.numeric()
n_k_t = rep(0,k)
for (t in 1:T) {
if (!is.null(c)) {
eta_t = c/t
} else {
eta_t = 0.9
}
probs = exp(eta_t*muEstimates)
if(any(is.na(probs))) {
print(probs)
print(muEstimates)
muEstimates = normalize(muEstimates)
print(muEstimates)
probs = exp(eta_t*muEstimates)/sum(exp(eta_t*muEstimates))
print(probs)
}
armChosen[t] = sample(seq(1,k,1), size = 1, prob = probs)
rewards[t] = rnorm(n=1,mean = mus[armChosen[t]], sd = sqrt(sigma2))
n_k_t[armChosen[t]] = n_k_t[armChosen[t]] + 1
muEstimates[armChosen[t]] = sum(rewards[armChosen == armChosen[t]])/(n_k_t[armChosen[t]] + max(0, 1 - n_k_t[armChosen[t]]))
}
data = data.frame(armChosen, rewards)
colnames(data) = c("ArmChosen", "Rewards")
return(data)
}
UCB = function(k, mus, sigma2) {
muEstimates = rep(0.5, k)
# Initialize vectors to store information
rewards = as.numeric()
armChosen = as.numeric()
n_k_t = rep(0,k)
t = 1
while (t <= T) {
# Generate the ucbs. We need the indices for this, hence the for loop.
ucbs = as.numeric()
for (i in 1:k) {
ucbs[i] = muEstimates[i] + sqrt(2*log(t)/(n_k_t[i] + max(0, 1 - n_k_t[i])))
}
# Choose arm from the one with highest UCB
armChosen[t] = which.max(ucbs)
# update n_k_t
n_k_t[armChosen[t]] = n_k_t[armChosen[t]] + 1
# Sample reward
rewards[t] = rnorm(n=1, mean = mus[armChosen[t]], sd = sqrt(sigma2))
# Update muEstimates
muEstimates[armChosen[t]] = sum(rewards[armChosen == armChosen[t]])/(n_k_t[armChosen[t]] + max(0, 1 - n_k_t[armChosen[t]]))
t = t + 1
}
data = data.frame(armChosen, rewards)
colnames(data) = c("ArmChosen", "Rewards")
return(data)
}
calc_regret = function(muStar, armChosen, mus) {
return(apply(as.matrix(armChosen), 2, function(row, muStar) {
return(muStar - mus[armChosen])
}, muStar))
}
cumRegret = function(regrets) {
cumRegrets = as.numeric()
for (i in 1:length(regrets)) {
cumRegrets[i] = sum(regrets[1:i])
}
return(cumRegrets)
}
analyze = function(k, sigma2, regretsEps1, regretsEps2, regretsEps3, regretsUCB, regretsSM1, regretsSM10, avgBestArmEps1, avgBestArmEps2, avgBestArmEps3, avgBestArmUCB, avgBestArmSM1, avgBestArmSM10) {
xSeq = seq(1, length(regretsEps1),1)
plot(x=xSeq, y = Convergence_Means(regretsEps1), type = "l", lwd = 2, xlab = "Iteraciones", ylab = "Arrepentimiento promedio", col = "blue", ylim = c(0,max(Convergence_Means(regretsEps1), Convergence_Means(regretsUCB))),main = paste("K = ",k, ", ", expression(sigma), " = ", sigma2))
lines(Convergence_Means(regretsUCB), lwd = 2, col = "red")
lines(Convergence_Means(regretsEps2), lwd = 2, col = "green")
lines(Convergence_Means(regretsEps3), lwd = 2, col = "black")
lines(Convergence_Means(regretsSM1), lwd = 2, col = "purple")
lines(Convergence_Means(regretsSM10), lwd = 2, col = "yellow")
cumulativeRegretEpsilon1 = cumRegret(regretsEps1)
cumulativeRegretEpsilon2 = cumRegret(regretsEps2)
cumulativeRegretEpsilon3 = cumRegret(regretsEps3)
cumulativeRegretUCV = cumRegret(regretsUCB)
cumulativeRegretSM1 = cumRegret(regretsSM1)
cumulativeRegretSM10 = cumRegret(regretsSM10)
legend("top",
col = c("blue", "red", "green", "black", "purple", "yellow"),
lwd = c(2,2),
legend = c(expression(paste(epsilon,"-greedy, ", epsilon, " = 0.5")),
"UCB",
expression(paste(epsilon," = 1/",sqrt(t))),
expression(paste(epsilon, "= 1/log(t)")),
expression(paste("Softmax, ",eta," = 0.9")),
expression(paste("Softmax, ",eta," = 10/t"))
))
plot(x=xSeq, y = cumulativeRegretEpsilon1, type = "l", lwd = 2, xlab = "Iteraciones", ylab = "Arrepentimiento cumulativo", col = "blue", ylim = c(min(cumulativeRegretUCV, cumulativeRegretEpsilon1),max(cumulativeRegretUCV, cumulativeRegretEpsilon1)),main = paste("K = ",k, ", ", expression(sigma), " = ", sigma2))
lines(cumulativeRegretUCV, lwd = 2, col = "red")
lines(cumulativeRegretEpsilon2, lwd = 2, col = "green")
lines(cumulativeRegretEpsilon3, lwd = 2, col = "black")
lines(cumulativeRegretSM1, lwd = 2, col = "purple")
lines(cumulativeRegretSM10, lwd = 2, col = "yellow")
legend("top",
col = c("blue", "red", "green", "black", "purple", "yellow"),
lwd = c(2,2),
legend = c(expression(paste(epsilon,"-greedy, ", epsilon, " = 0.5")),
"UCB",
expression(paste(epsilon," = 1/",sqrt(t))),
expression(paste(epsilon, "= 1/log(t)")),
expression(paste("Softmax, ",eta," = 0.9")),
expression(paste("Softmax, ",eta," = 10/t"))
))
## Plot best arm
plot(x=xSeq, y = avgBestArmEps1, type = "l", lwd = 2, xlab = "Iteraciones", ylab = "Veces mejor brazo elegido", col = "blue", ylim = c(0,max(avgBestArmEps1, avgBestArmEps2, avgBestArmEps3, avgBestArmUCB)),main = paste("K = ",k, ", ", expression(sigma), " = ", sigma2))
lines(avgBestArmUCB, lwd = 2, col = "red")
lines(avgBestArmEps2, lwd = 2, col = "green")
lines(avgBestArmEps3, lwd = 2, col = "black")
lines(avgBestArmSM1, lwd = 2, col = "purple")
lines(avgBestArmSM10, lwd = 2, col = "yellow")
legend("top",
col = c("blue", "red", "green", "black", "purple", "yellow"),
lwd = c(2,2),
legend = c(expression(paste(epsilon,"-greedy, ", epsilon, " = 0.5")),
"UCB",
expression(paste(epsilon," = 1/",sqrt(t))),
expression(paste(epsilon, "= 1/log(t)")),
expression(paste("Softmax, ",eta," = 0.9")),
expression(paste("Softmax, ",eta," = 10/t"))
))
}
bestArmPlayed = function(armPlayed, mus) {
percArmPlayed = as.numeric()
for (i in 1:length(armPlayed)) {
tmp = armPlayed[1:i]
percArmPlayed[i] = sum(tmp == which.max(mus))
}
return(percArmPlayed)
}
####################################
######## Initialize problem ########
####################################
set.seed(123)
# Set parameters
K = c(5,10,25,50)
sigma2 = c(0.01^2,0.1^2,1^2, 10^2)
## Set algorithm specifications
# Set epsilon mode. Sets which way the epsilon is decided. 1 for fixed, 2 for epsilon = 1/t, 3 for epsilon = 1/sqrt(t), 4 for epsilon = 1/log(t)
epsilonMode = 1
c = 1
#Setting number of max iterations and convergence criteria. Problem will run until convergence or for the max nr iterations specified.
T = 2000
nRuns = 1
convergence = Get_Convergence(mus)
# Initialize result vectors
# Generate results
for (k in K) {
for (sig in sigma2) {
for (i in 1:nRuns) {
# Get results for the run in terms of ArmChosen and reward
mus = runif(k, min = 0, max = 1)
epsilonMode = 1
epsilonResults1 = epsilonGreedy(k, mus, sig)
epsilonMode = 2
epsilonResults2 = epsilonGreedy(k, mus, sig)
epsilonMode = 3
epsilonResults3 = epsilonGreedy(k, mus, sig)
UCBResults = UCB(k, mus, sig)
if (sig != 10^2) {
softMaxC1 = SoftMax(k, mus, sig)
softMaxC10 = SoftMax(k, mus, sig, c = 10)
}
# Now, convert to AVERAGE regret over the 1000 iterations.
if (i == 1) {
avgRegretEps1 = calc_regret(mus[which.max(mus)], epsilonResults1$ArmChosen, mus)
avgRegretEps2 = calc_regret(mus[which.max(mus)], epsilonResults2$ArmChosen, mus)
avgRegretEps3 = calc_regret(mus[which.max(mus)], epsilonResults3$ArmChosen, mus)
avgRegretUCB = calc_regret(mus[which.max(mus)], UCBResults$ArmChosen, mus)
avgRegretSM1 = calc_regret(mus[which.max(mus)], softMaxC1$ArmChosen, mus)
avgRegretSM10 = calc_regret(mus[which.max(mus)], softMaxC10$ArmChosen, mus)
avgBestArmEps1 = bestArmPlayed(epsilonResults1$ArmChosen, mus)
avgBestArmEps2 = bestArmPlayed(epsilonResults2$ArmChosen, mus)
avgBestArmEps3 = bestArmPlayed(epsilonResults3$ArmChosen, mus)
avgBestArmUCB = bestArmPlayed(UCBResults$ArmChosen, mus)
avgBestArmSM1 = bestArmPlayed(softMaxC1$ArmChosen, mus)
avgBestArmSM10 = bestArmPlayed(softMaxC10$ArmChosen, mus)
} else {
avgRegretEps1 = avgRegretEps1*(i-1)/i + calc_regret(mus[which.max(mus)], epsilonResults1$ArmChosen, mus)/i
avgRegretEps2 = avgRegretEps2*(i-1)/i + calc_regret(mus[which.max(mus)], epsilonResults2$ArmChosen, mus)/i
avgRegretEps3 = avgRegretEps3*(i-1)/i + calc_regret(mus[which.max(mus)], epsilonResults3$ArmChosen, mus)/i
avgRegretUCB = avgRegretUCB*(i-1)/i + calc_regret(mus[which.max(mus)], UCBResults$ArmChosen, mus)/i
avgRegretSM1 = avgRegretSM1*(i-1)/i + calc_regret(mus[which.max(mus)], softMaxC1$ArmChosen, mus)/i
avgRegretSM10 = avgRegretSM10*(i-1)/i + calc_regret(mus[which.max(mus)], softMaxC10$ArmChosen, mus)/i
avgBestArmEps1 = avgBestArmEps1*(i-1)/i + bestArmPlayed(epsilonResults1$ArmChosen,mus)/i
avgBestArmEps2 = avgBestArmEps2*(i-1)/i + bestArmPlayed(epsilonResults2$ArmChosen,mus)/i
avgBestArmEps3 = avgBestArmEps3*(i-1)/i + bestArmPlayed(epsilonResults3$ArmChosen,mus)/i
avgBestArmUCB = avgBestArmUCB*(i-1)/i + bestArmPlayed(UCBResults$ArmChosen,mus)/i
avgBestArmSM1 = avgBestArmSM1*(i-1)/i + bestArmPlayed(softMaxC1$ArmChosen,mus)/i
avgBestArmSM10 = avgBestArmSM10*(i-1)/i + bestArmPlayed(softMaxC10$ArmChosen,mus)/i
}
print(i)
}
analyze(k, sqrt(sig), avgRegretEps1, avgRegretEps2, avgRegretEps3, avgRegretUCB, avgRegretSM1, avgRegretSM10, avgBestArmEps1, avgBestArmEps2, avgBestArmEps3, avgBestArmUCB, avgBestArmSM1, avgBestArmSM10)
}
}
data = read.csv("australian-crabs.csv")
DiscFunc = function(x, cov_k, mu_k, prior) {
x = as.matrix(x)
mu_k = as.matrix(mu_k)
express = t(x)%*%solve(cov_k)%*%mu_k -
(1/2)%*%t(mu_k)%*%solve(cov_k)%*%mu_k + log(prior)
return(express)
}
classificationRate = function(confMatrix) {
return(sum(diag(confMatrix))/sum(confMatrix))
}
# This function assumes only two dimensions of the data!
# Send in data as features as columns, data points as rows
LDA = function(x1, x2, prior1, prior2, test) {
mu_1 = apply(x1, 2, mean)
mu_2 = apply(x2, 2, mean)
cov_1 = cov(x1)
cov_2 = cov(x2)
n1 = nrow(x1)
n2 = nrow(x2)
N = n1+n2
covM = 1/N*(n1*cov_1 + n2*cov_2)
classificationsTest = apply(test, 1, function(row) {
class1 = DiscFunc(row, covM, mu_1, prior1)
class2 = DiscFunc(row, covM, mu_2, prior2)
return(which.max(c(class1,class2)))
})
classificationsTest = factor(ifelse(classificationsTest == 2, "Male", "Female"))
# Is proportional to, so we do not get the constant needed.
decisionBoundary = solve(covM)%*%(mu_2 - mu_1)
return(list(classificationsTest, decisionBoundary))
}
x2 = matrix(c(data[data$sex == "Male",]$CL,
data[data$sex == "Male",]$RW),
nrow = nrow(data[data$sex == "Male",]), ncol = 2)
x1 = matrix(c(data[data$sex == "Female",]$CL,
data[data$sex == "Female",]$RW),
nrow = nrow(data[data$sex == "Female",]), ncol = 2)
test_m = matrix(c(data$CL, data$RW), nrow = nrow(data), ncol = 2)
result = LDA(x1,x2,prior1 = 0.5,prior2 = 0.5, test = test_m)
classificationsTest = result[[1]]
decisionB = result[[2]]
print("Misclassification rate test")
table(data$sex, classificationsTest)
1 - classificationRate(table(data$sex, classificationsTest))
library(MASS)
fitModel = lda(formula = sex ~ RW + CL,
data = data,
prior = c(length(data$sex[data$sex == "Male"])
/nrow(data),length(data$sex[data$sex == "Female"])
/nrow(data)))
fits = predict(fitModel, data)
confMatrix = table(data$sex, fits$class)
confMatrix
print("misclassificionation rate test")
1 - classificationRate(confMatrix)
plot(data[data$sex == "Male",]$CL,
data[data$sex == "Male",]$RW,
col = "green", xlab = "CL", ylab = "RW")
points(data[data$sex == "Female",]$CL,
data[data$sex == "Female",]$RW, col = "red")
xSeq = seq(min(data$CL), max(data$CL))
c = 1.7 # How find constant c?
RWvals = -(decisionB[1,]*xSeq/decisionB[2,]) + c
lines(x=xSeq, y = RWvals)
## Special task 4
classificationRate = function(confMatrix) {
return(sum(diag(confMatrix))/sum(confMatrix))
}
data = read.csv("australian-crabs.csv")
n=dim(data)[1]
set.seed(12345)
id=sample(1:n, floor(n*0.5))
train=data[id,]
test=data[-id,]
NaiveBayesImpl = function(train, test) {
# Divide into subsets of the classes
trainBlue = train[train$species == "Blue",]
trainOrange = train[train$species == "Orange",]
# Set priors proportional to the class sizes
priorBlue = nrow(trainBlue)/(nrow(trainBlue) + nrow(trainOrange))
priorOrange = 1 - priorBlue
# Estimate the densities for the training sets
# through density() function for each class for each measurement variable.
# Could definitely be done smoother than this, but this works too.
BlueCLDens = density(trainBlue$CL)
OrangeCLDens = density(trainOrange$CL)
BlueRWDens = density(trainBlue$RW)
OrangeRWDens = density(trainOrange$RW)
BlueFLDens = density(trainBlue$FL)
OrangeFLDens = density(trainOrange$FL)
BlueCWDens = density(trainBlue$CW)
OrangeCWDens = density(trainOrange$CW)
BlueBDDens = density(trainBlue$BD)
OrangeBDDens = density(trainOrange$BD)
trainFeats = as.matrix(train[,-c(1,2,3)])
testFeats = as.matrix(test[,-c(1,2,3)])
# Classify them according to unnormalized probabilities
#obtained through densities calculated above.
# This assumes conditional
#independence of the variables, just like Naive Bayes
trainClassifications = factor(apply(trainFeats, 1, function(row) {
blue = approx(BlueFLDens$x, BlueFLDens$y, xout = row[1])$y*
approx(BlueRWDens$x, BlueRWDens$y, xout = row[2])$y*
approx(BlueCLDens$x,BlueCLDens$y,xout=row[3])$y*
approx(BlueCWDens$x, BlueCWDens$y, xout = row[4])$y*
approx(BlueBDDens$x, BlueBDDens$y, xout = row[5])$y*
priorBlue
orange = approx(OrangeFLDens$x, OrangeFLDens$y, xout = row[1])$y*
approx(OrangeRWDens$x, OrangeRWDens$y, xout = row[2])$y*
approx(OrangeCLDens$x,OrangeCLDens$y,xout=row[3])$y*
approx(OrangeCWDens$x, OrangeCWDens$y, xout = row[4])$y*
approx(OrangeBDDens$x, OrangeBDDens$y, xout = row[5])$y*
priorOrange
return(ifelse(blue >= orange, "blue", "orange"))
}))
print("Confusion matrix train")
print(table(train$species, trainClassifications))
print("Misclassification rate train")
print(1 - classificationRate(table(train$species, trainClassifications)))
testClassifications = factor(apply(testFeats, 1, function(row) {
blue = approx(BlueFLDens$x, BlueFLDens$y, xout = row[1])$y*
approx(BlueRWDens$x, BlueRWDens$y, xout = row[2])$y*
approx(BlueCLDens$x,BlueCLDens$y,xout=row[3])$y*
approx(BlueCWDens$x, BlueCWDens$y, xout = row[4])$y*
approx(BlueBDDens$x, BlueBDDens$y, xout = row[5])$y*
priorBlue
orange = approx(OrangeFLDens$x, OrangeFLDens$y, xout = row[1])$y*
approx(OrangeRWDens$x, OrangeRWDens$y, xout = row[2])$y*
approx(OrangeCLDens$x,OrangeCLDens$y,xout=row[3])$y*
approx(OrangeCWDens$x, OrangeCWDens$y, xout = row[4])$y*
approx(OrangeBDDens$x, OrangeBDDens$y, xout = row[5])$y*
priorOrange
return(ifelse(blue >= orange, "blue", "orange"))
}))
print("Confusion matrix test")
print(table(test$species, trainClassifications))
print("Misclassification rate test")
print(1 - classificationRate(table(test$species, trainClassifications)))
}
NaiveBayesImpl(train, test)
dataBlue = data[data$species = "Blue",]
dataBlue = data[data$species == "Blue",]
dataOrange = data[data$species == "Orange",]
cov(as.matrix(data[,-c(1,2,3)]))
dataBlue = data[data$species == "Blue",-c(1,2,3)]
dataOrange = data[data$species == "Orange",-c(1,2,3)]
cov(as.matrix(data[,-c(1,2,3)]))
cov(as.matrix(dataBlue))
cov(as.matrix(dataOrange))
dataBlue = data[data$species == "Blue",-c(1,2,3)]
dataOrange = data[data$species == "Orange",-c(1,2,3)]
cov(as.matrix(data[,-c(1,2,3)]))
"Covariance blue species"
cov(as.matrix(dataBlue))
"Covariance orange species"
cov(as.matrix(dataOrange))
dataBlue = data[data$species == "Blue",-c(1,2,3)]
dataOrange = data[data$species == "Orange",-c(1,2,3)]
cov(as.matrix(data[,-c(1,2,3)]))
"Covariance blue species"
cov(as.matrix(dataBlue))
"Covariance orange species"
cov(as.matrix(dataOrange))
dataBlue = data[data$species == "Blue",-c(1,2,3)]
dataOrange = data[data$species == "Orange",-c(1,2,3)]
"Covariance all species"
cov(as.matrix(data[,-c(1,2,3)]))
"Covariance blue species"
cov(as.matrix(dataBlue))
"Covariance orange species"
cov(as.matrix(dataOrange))
library(Filips.ML.package)
EuclidDistanceMatrix(samples,samples)
detach("package:Filips.ML.package", unload=TRUE)
library("Filips.ML.package", lib.loc="/Library/Frameworks/R.framework/Versions/3.4/Resources/library")
EuclidDistanceMatrix(samples,samples)
library(mvtnorm)
library(Filips.ML.package)
EuclidDistanceMatrix = function(X, Y) {
X = as.matrix(train)
Y = as.matrix(test)
Xhat = X/sqrt(rowSums(X^2))
Yhat = Y/sqrt(rowSums(Y^2))
C_matrix = Xhat%*%t(Yhat)
return(1 - C_matrix)
}
GenerateClusters = function(avgSampPerCluster, clusterVar, nrClusters, featureMins, featureMaxes, seed = 123) {
set.seed(seed)
df = data.frame(matrix(rep(0,length(featureMins)), ncol = length(featureMins), nrow = 1))
names = colnames(df)
for (i in 1:nrClusters) {
means = as.numeric()
vars = as.numeric()
for (j in 1:length(featureMins)) {
means[j] = runif(1, min = featureMins[j], max = featureMaxes[j])
vars[j] = abs(featureMins[j] - featureMaxes[j])
}
nrInCluster = floor(rnorm(1, avgSampPerCluster, clusterVar))
print(nrInCluster)
cluster = rmvnorm(nrInCluster, mean = means, sigma = diag(vars))
colnames(cluster) = names
df = rbind(df, cluster)
}
df = df[-c(1),]
return(df)
}
standardizeDataframe = function(df) {
names = colnames(df)
dataframe = data.frame(apply(df, 2, function(col) {
return(standardizeFeature(col))
}))
colnames(dataframe) = names
return(dataframe)
}
euclidDistAlgo = function(initCenters, features) {
# use euclidian distance!!
}
#Just start simple with this one
# features is a data frame with the features
LLoydsAlgorithm = function(k, features, seed = 123) {
X = as.matrix(features)
# N = number of data points in data set.
N = nrow(features)
# Initialize assignments uniformly at randomly
set.seed(seed)
assignments = sample(x = seq(1,k,1), replace = TRUE, size = N)
# initialize mu based on random assignments
mus = apply(as.matrix(seq(1,k,1)), 1, function(k_i, X) {
return((1/length(X[assignments == k_i,]))*colSums(X[assignments == k_i,]))
}, X)
print(mus)
#delete this line later
points(mus[1,], mus[2,], col = "green", pch = "X")
#While not converged
#Assign each point xi to closest center
#Update center as mean of assigned data points
converged = FALSE
count = 0
while (converged == FALSE) {
converged = TRUE
DistanceMatrix = EuclidDistanceMatrix(X, t(mus))
for (i in 1:N) {
# Check which is the closest mu to point. Use euclidian distance instead!!!  Much less computationally heavy.
# closestMu = which.min(apply(X, 1, function(dataPoint, mus) {
#   return(apply(as.matrix(mus), 2, function(mu, dataPoint) {
#     #print(sum((dataPoint - mu)^2))
#     return(sum((dataPoint - mu)^2))
#   }, dataPoint))
# }, mus)[,i])
#closestMus =
closestMu = which.min(DistanceMatrix[i,])
if (closestMu != assignments[i]) {
print(i)
converged = FALSE
assignments[i] = closestMu
}
}
count = count + 1
print(count)
}
# Recalculate mus
mus = apply(as.matrix(seq(1,k,1)), 1, function(index) {
return((1/nrow(X[assignments == index,]))*colSums(X[assignments == index,]))
})
#mus[,closestMu] = (1/nrow(pointsClosest))*colSums(pointsClosest)
points(mus[1,], mus[2,], col = "green", pch = "X")
return(mus)
}
KMeansOnline = function(initCenters, features) {
}
#Fix coresets functions
samples = GenerateClusters(300, 40, 6, c(-100,-100), c(100,100))
samples = standardizeDataframe(samples)
plot(samples$X1, samples$X2)
kMeans = LLoydsAlgorithm(k = 5, samples)
detach("package:Filips.ML.package", unload=TRUE)
#https://hilaryparker.com/2014/04/29/writing-an-r-package-from-scratch/
# create("Filips.ML.package")
# setwd("./cats")
# document()
#' When update to package is done (for Filips ML package):
#' 1. Set working directory to package.
#' 2. setwd("~/Desktop/Programmeringsprojekt/Machine Learning/Filips ML package/Filips.ML.package")
#'    document()
#' 3. Set working directory to folder above and reinstall the package with updates.
#'    setwd("~/Desktop/Programmeringsprojekt/Machine Learning/Filips ML package")
#'    install("Filips.ML.package")
#'
# To use the ML functions, run the code below.
library("devtools")
library(roxygen2)
setwd("/Users/filipcornell/Desktop/Programmeringsprojekt/ML-Implementations/Filips ML package/Filips.ML.package")
document()
setwd("/Users/filipcornell/Desktop/Programmeringsprojekt/ML-Implementations/Filips ML package")
install("Filips.ML.package")
#this.dir <- dirname(parent.frame(2)$ofile) #To set it to source file location
#setwd(this.dir) #To set it to source file location
#library("Filips.ML.package")
