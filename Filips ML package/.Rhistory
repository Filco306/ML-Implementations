dataBlue = data[data$species = "Blue",]
dataBlue = data[data$species == "Blue",]
dataOrange = data[data$species == "Orange",]
cov(as.matrix(data[,-c(1,2,3)]))
dataBlue = data[data$species == "Blue",-c(1,2,3)]
dataOrange = data[data$species == "Orange",-c(1,2,3)]
cov(as.matrix(data[,-c(1,2,3)]))
cov(as.matrix(dataBlue))
cov(as.matrix(dataOrange))
dataBlue = data[data$species == "Blue",-c(1,2,3)]
dataOrange = data[data$species == "Orange",-c(1,2,3)]
cov(as.matrix(data[,-c(1,2,3)]))
"Covariance blue species"
cov(as.matrix(dataBlue))
"Covariance orange species"
cov(as.matrix(dataOrange))
dataBlue = data[data$species == "Blue",-c(1,2,3)]
dataOrange = data[data$species == "Orange",-c(1,2,3)]
cov(as.matrix(data[,-c(1,2,3)]))
"Covariance blue species"
cov(as.matrix(dataBlue))
"Covariance orange species"
cov(as.matrix(dataOrange))
dataBlue = data[data$species == "Blue",-c(1,2,3)]
dataOrange = data[data$species == "Orange",-c(1,2,3)]
"Covariance all species"
cov(as.matrix(data[,-c(1,2,3)]))
"Covariance blue species"
cov(as.matrix(dataBlue))
"Covariance orange species"
cov(as.matrix(dataOrange))
setwd("~/Desktop/Plugg_Lkpg/HT18/TDDE01/Lab 2")
classificationRate = function(confMatrix) {
return(sum(diag(confMatrix))/sum(confMatrix))
}
log_reg = function(data, fitModel, pLimit = 0.5) {
# Specify response in predict,
fits = predict(fitModel, data)
probabilities = fits
classifications = apply(as.matrix(probabilities), 1, function(row) {
if (row > pLimit) {
return(1)
} else {
return(0)
}
})
return(classifications)
}
library(mvtnorm)
library(Filips.ML.package)
#Fix coresets functions
samples = GenerateClusters(1000, 40, 10, c(-100,-100, -100), c(100,100, 100), seed = 12)
samples = standardizeData(as.matrix(samples), typeIn = "Matrix")
kMeans = LLoydsAlgorithm(k = 9, samples, convergenceCheckType = "Threshold")
kMeanOnline = KMeansOnline(k = 9, samples)
library(mvtnorm)
library(Filips.ML.package)
#Fix coresets functions
samples = GenerateClusters(1000, 40, 10, c(-100,-100, -100), c(100,100, 100), seed = 12)
samples = standardizeData(as.matrix(samples), typeIn = "Matrix")
kMeans = LLoydsAlgorithm(k = 9, samples, convergenceCheckType = "Threshold")
plot(samples$X1, samples$X2, col = "red")
points(kMeans[,1], kMeans[,2], col = "blue", pch = "X")
setwd("~/Desktop/Plugg_Lkpg/HT18/TDDE01/Lab 2")
classificationRate = function(confMatrix) {
return(sum(diag(confMatrix))/sum(confMatrix))
}
log_reg = function(data, fitModel, pLimit = 0.5) {
# Specify response in predict,
fits = predict(fitModel, data)
probabilities = fits
classifications = apply(as.matrix(probabilities), 1, function(row) {
if (row > pLimit) {
return(1)
} else {
return(0)
}
})
return(classifications)
}
# Task 1
data = read.csv("australian-crabs.csv")
# Should I use training and test? Probably yes
n=dim(data)[1]
set.seed(12345)
id=sample(1:n, floor(n*0.5))
train=data[id,]
test=data[-id,]
plot(data[data$sex == "Male",]$CL,
data[data$sex == "Male",]$RW, col = "green")
points(data[data$sex == "Female",]$CL,
data[data$sex == "Female",]$RW, col = "red")
legend("right",
pch = c("o","o"),
col = c("green", "red"),
legend = c("Male","Female"))
# Task 2
library(MASS)
fitModel = lda(formula = sex ~ RW + CL,
data = data,
prior = c(length(data$sex[data$sex == "Male"])
/nrow(data),length(data$sex[data$sex == "Female"])
/nrow(data)))
fits = predict(fitModel, data)
par(mfrow=c(2,1))
plot(data[data$sex == "Male",]$CL,
data[data$sex == "Male",]$RW,
col = "green", ylab = "RW", xlab = "CL")
points(data[data$sex == "Female",]$CL,
data[data$sex == "Female",]$RW, col = "red")
plot(data[fits$class == "Male",]$CL,
data[fits$class == "Male",]$RW,
col = "green", ylab = "RW", xlab = "CL")
points(data[fits$class == "Female",]$CL,
data[fits$class == "Female",]$RW,
col = "red")
par(mfrow=c(1,1))
confMatrix = table(data$sex, fits$class)
confMatrix
print("misclassificionation rate")
1 - classificationRate(confMatrix)
# Task 3
fitModel = lda(formula = sex ~ RW + CL, data = data, prior = c(0.1, 0.9))
fits = predict(fitModel, data)
par(mfrow=c(2,1))
plot(data[data$sex == "Male",]$CL,
data[data$sex == "Male",]$RW, col = "green")
points(data[data$sex == "Female",]$CL,
data[data$sex == "Female",]$RW, col = "red")
plot(data[fits$class == "Male",]$CL,
data[fits$class == "Male",]$RW, col = "green")
points(data[fits$class == "Female",]$CL,
data[fits$class == "Female",]$RW, col = "red")
par(mfrow=c(1,1))
confMatrix = table(data$sex, fits$class)
confMatrix
print("misclassificionation rate")
1 - classificationRate(confMatrix)
# Task 4
fit = glm(sex ~ RW + CL, data = data, family = "binomial")
#summary(fit)
fits = predict(fit, data)
probabilities = fits
pLimit = 0.5
classifications = apply(as.matrix(probabilities), 1, function(row) {
if (row > pLimit) {
return(1)
} else {
return(0)
}
})
#classificationsTest = log_reg(test, fit)
table(data$sex, classifications)
print("misclassification rate")
1 - classificationRate(table(data$sex, classifications))
fit$coefficients
# Report the equation of the decision boundary and draw it in the plot of the classified data
plot(data[classifications == 1,]$CL,
data[classifications == 1,]$RW,
col = "green", xlab = "CL", ylab ="RW")
points(data[classifications == 0,]$CL,
data[classifications == 0,]$RW, col = "red")
# Equation of decision boundary: $0.5 = \frac{1}{1 + exp(-(intercept + beta1*RW + beta2*CL))}$
CLseq = seq(min(data$CL), max(data$CL), length = 1000)
Xseq = matrix(c(rep(1, length = 1000),CLseq), ncol = 2, nrow = 1000)
RWseq = -t(as.matrix(c(fit$coefficients[1],
fit$coefficients[3])))%*%t(Xseq)/fit$coefficients[2]
lines(x = CLseq, y = RWseq, lwd = 3)
classificationRate = function(confMatrix) {
return(sum(diag(confMatrix))/sum(confMatrix))
}
library(readxl)
data = read_excel("creditscoring.xls")
data$good_bad = factor(data$good_bad)
# Task 1
n=dim(data)[1]
set.seed(12345)
id=sample(1:n, floor(n*0.5))
train=data[id,]
id1=setdiff(1:n, id)
set.seed(12345)
id2=sample(id1, floor(n*0.25))
valid=data[id2,]
id3=setdiff(id1,id2)
test=data[id3,]
#install.packages("tree")
#library(rpart)
library(tree)
# Fitting with gini
fitGini = tree(good_bad ~., data = train, split = c("gini"))
trainClassifGini = factor(apply(predict(fitGini, train), 1, function(row) {
return(names(which.max(row)))
}))
testClassifGini = factor(apply(predict(fitGini, test), 1, function(row) {
return(names(which.max(row)))
}))
table(train$good_bad, trainClassifGini)
print("misclassification rate train gini")
1 - classificationRate(table(train$good_bad, trainClassifGini))
table(test$good_bad, testClassifGini)
print("misclassification rate test gini")
1 - classificationRate(table(test$good_bad, testClassifGini))
# fitting with deviance
fitDeviance = tree(good_bad ~., data = train, split = c("deviance"))
trainClassifDeviance = factor(apply(predict(fitDeviance, train), 1, function(row) {
return(names(which.max(row)))
}))
testClassifDeviance = factor(apply(predict(fitDeviance, test), 1, function(row) {
return(names(which.max(row)))
}))
table(train$good_bad, trainClassifDeviance)
print("misclassification rate train deviance")
1 - classificationRate(table(train$good_bad, trainClassifDeviance))
table(test$good_bad, testClassifDeviance)
print("misclassification rate test deviance")
1 - classificationRate(table(test$good_bad, testClassifDeviance))
#Deviance is best => choose deviance!
# Task 3
fit = fitDeviance
trainScore=rep(0,9)
testScore=rep(0,9)
for(i in 2:9) {
prunedTree=prune.tree(fit,best=i)
pred=predict(prunedTree, newdata=valid,
type="tree")
trainScore[i]=deviance(prunedTree)
testScore[i]=deviance(pred)
}
plot(2:9, trainScore[2:9], type="b", col="red",
ylim=c(0,max(trainScore, testScore)), xlab = "Depth", ylab = "Score")
points(2:9, testScore[2:9], type="b", col="blue")
depth = which.min(testScore[2:9]) + 1
print(paste("Best depth is", depth))
finalTree=prune.tree(fit, best=depth)
Yfit=predict(finalTree, newdata=valid,
type="class")
table(valid$good_bad,Yfit)
1 - classificationRate(table(valid$good_bad,Yfit))
print(finalTree)
# Task 3
fit = fitDeviance
trainScore=rep(0,9)
testScore=rep(0,9)
for(i in 2:9) {
prunedTree=prune.tree(fit,best=i)
pred=predict(prunedTree, newdata=valid,
type="tree")
trainScore[i]=deviance(prunedTree)
testScore[i]=deviance(pred)
}
plot(2:9, trainScore[2:9], type="b", col="red",
ylim=c(0,max(trainScore, testScore)), xlab = "Depth", ylab = "Score")
points(2:9, testScore[2:9], type="b", col="blue")
depth = which.min(testScore[2:9]) + 1
print(paste("Best depth is", depth))
finalTree=prune.tree(fit, best=depth)
Yfit=predict(finalTree, newdata=valid,
type="class")
table(valid$good_bad,Yfit)
1 - classificationRate(table(valid$good_bad,Yfit))
print(finalTree)
plot(tree)
# Task 3
fit = fitDeviance
trainScore=rep(0,9)
testScore=rep(0,9)
for(i in 2:9) {
prunedTree=prune.tree(fit,best=i)
pred=predict(prunedTree, newdata=valid,
type="tree")
trainScore[i]=deviance(prunedTree)
testScore[i]=deviance(pred)
}
plot(2:9, trainScore[2:9], type="b", col="red",
ylim=c(0,max(trainScore, testScore)), xlab = "Depth", ylab = "Score")
points(2:9, testScore[2:9], type="b", col="blue")
depth = which.min(testScore[2:9]) + 1
print(paste("Best depth is", depth))
finalTree=prune.tree(fit, best=depth)
Yfit=predict(finalTree, newdata=valid,
type="class")
table(valid$good_bad,Yfit)
1 - classificationRate(table(valid$good_bad,Yfit))
print(finalTree)
plot(finalTree)
# Task 3
fit = fitDeviance
trainScore=rep(0,9)
testScore=rep(0,9)
for(i in 2:9) {
prunedTree=prune.tree(fit,best=i)
pred=predict(prunedTree, newdata=valid,
type="tree")
trainScore[i]=deviance(prunedTree)
testScore[i]=deviance(pred)
}
plot(2:9, trainScore[2:9], type="b", col="red",
ylim=c(0,max(trainScore, testScore)), xlab = "Depth", ylab = "Score")
points(2:9, testScore[2:9], type="b", col="blue")
depth = which.min(testScore[2:9]) + 1
print(paste("Best depth is", depth))
finalTree=prune.tree(fit, best=depth)
Yfit=predict(finalTree, newdata=valid,
type="class")
table(valid$good_bad,Yfit)
1 - classificationRate(table(valid$good_bad,Yfit))
print(finalTree)
plot(finalTree)
text(finalTree, pretty = 0)
# Task 4 - use naïve bayes
library(MASS)
#install.packages("e1071")
library(e1071)
fit = naiveBayes(good_bad ~.,data=train)
classTrain = predict(fit, train)
classTest = predict(fit, test)
table(train$good_bad, classTrain)
1 - classificationRate(table(train$good_bad, classTrain))
table(test$good_bad, classTest)
1 - classificationRate(table(test$good_bad, classTest))
# Task 3
fit = fitDeviance
trainScore=rep(0,9)
testScore=rep(0,9)
for(i in 2:9) {
prunedTree=prune.tree(fit,best=i)
pred=predict(prunedTree, newdata=valid,
type="tree")
trainScore[i]=deviance(prunedTree)
testScore[i]=deviance(pred)
}
plot(2:9, trainScore[2:9], type="b", col="red",
ylim=c(200,max(trainScore, testScore)), xlab = "Depth", ylab = "Score")
points(2:9, testScore[2:9], type="b", col="blue")
depth = which.min(testScore[2:9]) + 1
print(paste("Best depth is", depth))
finalTree=prune.tree(fit, best=depth)
Yfit=predict(finalTree, newdata=valid,
type="class")
table(valid$good_bad,Yfit)
1 - classificationRate(table(valid$good_bad,Yfit))
print(finalTree)
plot(finalTree)
text(finalTree, pretty = 0)
# Task 4 - use naïve bayes
library(MASS)
#install.packages("e1071")
library(e1071)
fit = naiveBayes(good_bad ~.,data=train)
classTrain = predict(fit, train)
classTest = predict(fit, test)
table(train$good_bad, classTrain)
1 - classificationRate(table(train$good_bad, classTrain))
table(test$good_bad, classTest)
1 - classificationRate(table(test$good_bad, classTest))
# Task 5 use optimal tree and naive bayes model to classify test data by using Y
Pi = as.matrix(seq(0,1, by = 0.05))
predictionsTest = predict(finalTree, test)
predictionsTestBayes = predict(fit, test, "raw")
testClassifs = matrix(apply(as.matrix(Pi), 1, function(pi_val, preds) {
return(ifelse(preds > pi_val, 2,1))
}, predictionsTest[,2]), nrow = nrow(predictionsTest),ncol = length(Pi))
testClassifsBayes = matrix(apply(as.matrix(Pi), 1, function(pi_val, preds) {
return(ifelse(preds > pi_val, 2,1))
}, predictionsTestBayes[,2]), nrow = nrow(predictionsTestBayes),ncol = length(Pi))
TPR = function(predictions, labels) {
N_plus = sum(labels == "good")
labels = as.numeric(labels)
return(sum(predictions == 2 & labels == predictions)/N_plus)
}
FPR = function(predictions, labels) {
N_minus = sum(labels == "bad")
labels = as.numeric(labels)
return(sum(predictions == 2 & labels != predictions)/N_minus)
}
TPRs = apply(testClassifs,2,TPR, test$good_bad)
FPRs = apply(testClassifs,2, FPR, test$good_bad)
plot(x=FPRs, y = TPRs,
type = "l", xlim = c(0,1), ylim = c(0,1),
lwd = 3, main = "ROC curves")
TPRsBayes = apply(testClassifsBayes,2,TPR, test$good_bad)
FPRsBayes = apply(testClassifsBayes,2,FPR, test$good_bad)
lines(x=FPRsBayes, y = TPRsBayes, col = "blue", lwd = 3)
legend("right",
lty = rep(1,2),
col = c("black", "blue"),
lwd = c(3,3),
legend = c("Optimal tree","Naïve Bayes"))
AUC = function(TPR, FPR) {
# TPR is y, FPR is x
# Order after FPR
xInd = order(FPR)
x = FPR[xInd]
y = TPR[xInd]
area = 0
for (i in 2:length(TPR)) {
area = (x[i]-x[i-1])*(y[i] + y[i-1])/2 + area
}
return(area)
}
print("AUC bayes")
AUC(TPRsBayes, FPRsBayes)
print("AUC tree")
AUC(TPRs, FPRs)
############################
####### ASSIGNMENT 4 #######
############################
data = read.csv("NIRSpectra.csv", sep = ";", dec = ",")
featSpace = data[,-c(ncol(data))]
result = prcomp(featSpace)
lambda=result$sdev^2
explanationFactor = lambda/sum(lambda)*100
names(explanationFactor) = seq(1,length(lambda),1)
plot(explanationFactor, type = "b", xlab = "Variable", ylab = "% explanatory")
explanationFactor = sort(explanationFactor, decreasing = T)
sumP = 0
i = 0
while (sumP < 99) {
i = i + 1
sumP = sumP + explanationFactor[i]
}
nrVarsToInvolve = i
plot(result$x[,1], result$x[,2], xlab = "PC1", ylab = "PC2")
classificationRate = function(confMatrix) {
return(sum(diag(confMatrix))/sum(confMatrix))
}
log_reg = function(data, fitModel, pLimit = 0.5) {
# Specify response in predict,
fits = predict(fitModel, data)
probabilities = fits
classifications = apply(as.matrix(probabilities), 1, function(row) {
if (row > pLimit) {
return(1)
} else {
return(0)
}
})
return(classifications)
}
screeplot(explanationFactor, xlab = "Variable", ylab = "% explanatory")
############################
####### ASSIGNMENT 4 #######
############################
data = read.csv("NIRSpectra.csv", sep = ";", dec = ",")
featSpace = data[,-c(ncol(data))]
result = prcomp(featSpace)
lambda=result$sdev^2
explanationFactor = lambda/sum(lambda)*100
names(explanationFactor) = seq(1,length(lambda),1)
screeplot(explanationFactor, xlab = "Variable", ylab = "% explanatory")
############################
####### ASSIGNMENT 4 #######
############################
data = read.csv("NIRSpectra.csv", sep = ";", dec = ",")
featSpace = data[,-c(ncol(data))]
result = prcomp(featSpace)
lambda=result$sdev^2
explanationFactor = lambda/sum(lambda)*100
names(explanationFactor) = seq(1,length(lambda),1)
screeplot(explanationFactor, type = "b", xlab = "Variable", ylab = "% explanatory")
############################
####### ASSIGNMENT 4 #######
############################
data = read.csv("NIRSpectra.csv", sep = ";", dec = ",")
featSpace = data[,-c(ncol(data))]
result = prcomp(featSpace)
lambda=result$sdev^2
explanationFactor = lambda/sum(lambda)*100
names(explanationFactor) = seq(1,length(lambda),1)
plot(explanationFactor, type = "b", xlab = "Variable", ylab = "% explanatory")
explanationFactor = sort(explanationFactor, decreasing = T)
sumP = 0
i = 0
while (sumP < 99) {
i = i + 1
sumP = sumP + explanationFactor[i]
}
nrVarsToInvolve = i
plot(result$x[,1], result$x[,2], xlab = "PC1", ylab = "PC2")
# Task 3: Perform independent component analysis
#install.packages("fastICA")
library(fastICA)
#fICA = fastICA(as.matrix(featSpace),)
#X = as.matrix()
# a. Compute W' = K * W
set.seed(12345)
fICAResult = fastICA(as.matrix(featSpace), n.comp = nrVarsToInvolve)
W_prime = fICAResult$K%*%fICAResult$W
plot(W_prime[,1], xlab = "Variable", ylab = "W'")
plot(W_prime[,2], xlab = "Variable", ylab = "W'")
#https://hilaryparker.com/2014/04/29/writing-an-r-package-from-scratch/
# create("Filips.ML.package")
# setwd("./cats")
# document()
#' When update to package is done (for Filips ML package):
#' 1. Set working directory to package.
#' 2. setwd("~/Desktop/Programmeringsprojekt/Machine Learning/Filips ML package/Filips.ML.package")
#'    document()
#' 3. Set working directory to folder above and reinstall the package with updates.
#'    setwd("~/Desktop/Programmeringsprojekt/Machine Learning/Filips ML package")
#'    install("Filips.ML.package")
#'
# To use the ML functions, run the code below.
library("devtools")
library(roxygen2)
setwd("/Users/filipcornell/Desktop/Programmeringsprojekt/ML-Implementations/Filips ML package/Filips.ML.package")
document()
setwd("/Users/filipcornell/Desktop/Programmeringsprojekt/ML-Implementations/Filips ML package")
install("Filips.ML.package")
#this.dir <- dirname(parent.frame(2)$ofile) #To set it to source file location
#setwd(this.dir) #To set it to source file location
#library("Filips.ML.package")
