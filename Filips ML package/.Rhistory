n = length(y)
return((1/n)*sum((y - y_hat)^2))
}
#install.packages("neuralnet")
library(neuralnet)
set.seed(1234567890)
Var <- runif(50, 0, 10)
trva <- data.frame(Var, Sin=sin(Var))
tr <- trva[1:25,] # Training
va <- trva[26:50,] # Validation
# Random initialization of the weights in the interval [-1, 1]
winit <- runif(50, min = -1, max = 1)
thr = as.numeric()
MSEsValidation = as.numeric()
MSEsTrain = as.numeric()
for (i in 1:10) {
thr[i] = i/1000
nn <- neuralnet(Sin ~ Var, data = tr, threshold = thr[i], startweights = winit, hidden = 10)
# Use validation set here to get error
yValidation = compute(nn, va$Var)$net.result
MSEsValidation[i] = MSE(y = va$Sin, y_hat = yValidation[,1])
yTrain = compute(nn, tr$Var)$net.result
MSEsTrain[i] = MSE(y = tr$Sin, y_hat = yTrain[,1])
#predict(nn, data = va$Var)
#plot(prediction(nn)$rep1)
}
# Reuse function created in lab 1
MSE = function(y, y_hat) {
n = length(y)
return((1/n)*sum((y - y_hat)^2))
}
#install.packages("neuralnet")
library(neuralnet)
set.seed(1234567890)
Var <- runif(50, 0, 10)
trva <- data.frame(Var, Sin=sin(Var))
tr <- trva[1:25,] # Training
va <- trva[26:50,] # Validation
# Random initialization of the weights in the interval [-1, 1]
winit <- runif(50, min = -1, max = 1)
thr = as.numeric()
MSEsValidation = as.numeric()
MSEsTrain = as.numeric()
for (i in 1:10) {
thr[i] = i/1000
nn <- neuralnet(Sin ~ Var, data = tr, threshold = thr[i], startweights = winit, hidden = 10)
# Use validation set here to get error
yValidation = compute(nn, va$Var)$net.result
MSEsValidation[i] = MSE(y = va$Sin, y_hat = yValidation[,1])
yTrain = compute(nn, tr$Var)$net.result
MSEsTrain[i] = MSE(y = tr$Sin, y_hat = yTrain[,1])
#predict(nn, data = va$Var)
#plot(prediction(nn)$rep1)
}
#plot(x = seq(1,10,1), y = MSEsValidation, type = "l", "MSE on validation set")
plot(x = thr, y = MSEsValidation, type = "l", lwd = 3, xlab = "Threshold", ylab = "MSE", main = "MSE on validation set")
lines(x = thr, y = MSEsTrain, col = "blue", lwd = 3)
# Choose the threshold giving the least amount of error.
bestThr = thr[which.min(MSEsValidation)]
# Plot the best one
plot(nn <- neuralnet(Sin ~ Var, data = tr, threshold = bestThr, startweights = winit, hidden = 10))
# Plot of the predictions (black dots) and the data (red dots)
plot(prediction(nn)$rep1)
points(trva, col = "red")
#plot(x = seq(1,10,1), y = MSEsValidation, type = "l", "MSE on validation set")
plot(x = thr, y = MSEsValidation, type = "l", lwd = 3, xlab = "Threshold", ylab = "MSE", main = "MSE on validation set")
lines(x = thr, y = MSEsTrain, col = "blue", lwd = 3)
plot(x = thr, y = MSEsValidation, type = "l", lwd = 3, xlab = "Threshold", ylab = "MSE", main = "MSE on validation set")
lines(x = thr, y = MSEsTrain, col = "blue", lwd = 3)
plot(x = thr, y = MSEsValidation, type = "l", lwd = 3, xlab = "Threshold", ylab = "MSE", main = "MSE on validation set", ylim = c(min(MSEsTrain, MSEsValidation), max(MSEsTrain, MSEsValidation)))
lines(x = thr, y = MSEsTrain, col = "blue", lwd = 3)
# Reuse function created in lab 1
MSE = function(y, y_hat) {
n = length(y)
return((1/n)*sum((y - y_hat)^2))
}
#install.packages("neuralnet")
library(neuralnet)
set.seed(1234567890)
Var <- runif(50, 0, 10)
trva <- data.frame(Var, Sin=sin(Var))
tr <- trva[1:25,] # Training
va <- trva[26:50,] # Validation
# Random initialization of the weights in the interval [-1, 1]
winit <- runif(50, min = -1, max = 1)
thr = as.numeric()
MSEsValidation = as.numeric()
MSEsTrain = as.numeric()
for (i in 1:10) {
thr[i] = i/1000
nn <- neuralnet(Sin ~ Var, data = tr, threshold = thr[i], startweights = winit, hidden = 10)
# Use validation set here to get error
yValidation = compute(nn, va$Var)$net.result
MSEsValidation[i] = MSE(y = va$Sin, y_hat = yValidation[,1])
yTrain = compute(nn, tr$Var)$net.result
MSEsTrain[i] = MSE(y = tr$Sin, y_hat = yTrain[,1])
#predict(nn, data = va$Var)
#plot(prediction(nn)$rep1)
}
#plot(x = seq(1,10,1), y = MSEsValidation, type = "l", "MSE on validation set")
plot(x = thr, y = MSEsValidation, type = "l", lwd = 3, xlab = "Threshold", ylab = "MSE", main = "MSE on validation set", ylim = c(min(MSEsTrain, MSEsValidation), max(MSEsTrain, MSEsValidation)))
lines(x = thr, y = MSEsTrain, col = "blue", lwd = 3)
# Choose the threshold giving the least amount of error.
bestThr = thr[which.min(MSEsValidation)]
# Plot the best one
plot(nn <- neuralnet(Sin ~ Var, data = tr, threshold = bestThr, startweights = winit, hidden = 10))
# Plot of the predictions (black dots) and the data (red dots)
plot(prediction(nn)$rep1)
points(trva, col = "red")
set.seed(1234567890)
Var <- runif(50, 0, 10)
trva <- data.frame(Var, Sin=sin(Var))
tr <- trva[1:25,] # Training
va <- trva[26:50,] # Validation
# plot(trva)
# plot(tr)
# plot(va)
w_j <- runif(10, -1, 1)
b_j <- runif(10, -1, 1)
w_k <- runif(10, -1, 1)
b_k <- runif(1, -1, 1)
l_rate <- 1/nrow(tr)^2
n_ite = 5000
error <- rep(0, n_ite)
error_va <- rep(0, n_ite)
tanh(seq(1,10,1))
seq(1,10,1)*seq(1,10,1)
for(i in 1:n_ite) {
# error computation: Your code here
cat("i: ", i, ", error: ", error[i]/2, ", error_va: ", error_va[i]/2, "\n")
flush.console()
for(n in 1:nrow(tr)) {
# forward propagation: Your code here
#forwardPropagation()
# only one variable, thus no need to sum up
# Step 1
z = tanh(w_j*tr$Var[n])
y = sum(w_k*z)
# backward propagation: Your code here
# Step 2
b_k = y - tr$Sin[n]
# Step 3
b_j = (1 - z^2)*b_k*w_k
}
}
for(i in 1:n_ite) {
# error computation: Your code here
cat("i: ", i, ", error: ", error[i]/2, ", error_va: ", error_va[i]/2, "\n")
flush.console()
for(n in 1:nrow(tr)) {
# forward propagation: Your code here
#forwardPropagation()
# only one variable, thus no need to sum up
# Step 1
z = tanh(w_j*tr$Var[n])
y = sum(w_k*z)
# backward propagation: Your code here
# Step 2
b_k = y - tr$Sin[n]
# Step 3
b_j = (1 - z^2)*b_k*w_k
}
}
a = seq(1,10,1)
typeof(a)
help(lda)
??lda
set.seed(123)
Iris <- data.frame(rbind(iris3[,,1], iris3[,,2], iris3[,,3]),
Sp = rep(c("s","c","v"), rep(50,3)))
train <- sample(1:150, 75)
table(Iris$Sp[train])
z <- lda(Sp ~ ., Iris, prior = c(1,1,1)/3, subset = train)
library(Filips.ML.package)
library(MASS)
set.seed(123)
Iris <- data.frame(rbind(iris3[,,1], iris3[,,2], iris3[,,3]),
Sp = rep(c("s","c","v"), rep(50,3)))
train <- sample(1:150, 75)
table(Iris$Sp[train])
z <- lda(Sp ~ ., Iris, prior = c(1,1,1)/3, subset = train)
predict(z, Iris[-train, ])$class
Iris[-train, ]
table(Iris[-train,]$Sp, preds)
preds = predict(z, Iris[-train, ])$class
table(Iris[-train,]$Sp, preds)
multiclassLDA = function(train, labelsTrain, priors, validation) {
# Create means for each class
levelsClasses = levels(labelsTrain)
# Will return means for each feature for each class.
mus = apply(as.matrix(levelsClasses), 1, function(class) {
return(mean(train[class = labelsTrain]))
})
# Extract covariance matrix for each class.
covs = apply(as.matrix(levelsClasses), 1, function(class) {
return(cov(train[class = labelsTrain]))
})
nC = apply(as.matrix(levelsClasses), 1, function(class) {
return(sum(class = labelsTrain))
})
N = sum(nC)
covM = 1/N*sum(nC*covs)
}
library(Filips.ML.package)
library(MASS)
multiclassLDA = function(train, labelsTrain, priors, validation) {
# Create means for each class
levelsClasses = levels(labelsTrain)
# Will return means for each feature for each class.
mus = apply(as.matrix(levelsClasses), 1, function(class) {
return(mean(train[class = labelsTrain]))
})
# Extract covariance matrix for each class.
covs = apply(as.matrix(levelsClasses), 1, function(class) {
return(cov(train[class = labelsTrain]))
})
nC = apply(as.matrix(levelsClasses), 1, function(class) {
return(sum(class = labelsTrain))
})
N = sum(nC)
covM = 1/N*sum(nC*covs)
}
# Trying with code and trying the LDA as instructed lda() function help example.
set.seed(123)
Iris <- data.frame(rbind(iris3[,,1], iris3[,,2], iris3[,,3]),
Sp = rep(c("s","c","v"), rep(50,3)))
train <- sample(1:150, 75)
table(Iris$Sp[train])
z <- lda(Sp ~ ., Iris, prior = c(1,1,1)/3, subset = train)
preds = predict(z, Iris[-train, ])$class
table(Iris[-train,]$Sp, preds)
Iris[train,]
Iris[train,-c(5)]
Iris[train,c(5)]
# Now trying my own implementation
multiclassLDA(train = Iris[train,-c(5)], labelsTrain = Iris[train,c(5)], priors = c(1,1,1)/3, validation = Iris[-train,])
multiclassLDA = function(train, labelsTrain, priors, validation) {
# Create means for each class
levelsClasses = levels(labelsTrain)
# Will return means for each feature for each class.
mus = apply(as.matrix(levelsClasses), 1, function(class) {
return(mean(train[class == labelsTrain]))
})
# Extract covariance matrix for each class.
covs = apply(as.matrix(levelsClasses), 1, function(class) {
return(cov(train[class == labelsTrain]))
})
nC = apply(as.matrix(levelsClasses), 1, function(class) {
return(sum(class = labelsTrain))
})
N = sum(nC)
covM = 1/N*sum(nC*covs)
}
# Now trying my own implementation
multiclassLDA(train = Iris[train,-c(5)], labelsTrain = Iris[train,c(5)], priors = c(1,1,1)/3, validation = Iris[-train,])
# Now trying my own implementation
multiclassLDA(train = Iris[train,-c(5)], labelsTrain = Iris[train,c(5)], priors = c(1,1,1)/3, validation = Iris[-train,])
multiclassLDA = function(train, labelsTrain, priors, validation) {
# Create means for each class
levelsClasses = levels(labelsTrain)
# Will return means for each feature for each class.
mus = apply(as.matrix(levelsClasses), 1, function(class) {
print(class)
print(labelsTrain)
return(mean(train[class == labelsTrain]))
})
# Extract covariance matrix for each class.
covs = apply(as.matrix(levelsClasses), 1, function(class) {
return(cov(train[class == labelsTrain]))
})
nC = apply(as.matrix(levelsClasses), 1, function(class) {
return(sum(class = labelsTrain))
})
N = sum(nC)
covM = 1/N*sum(nC*covs)
}
# Now trying my own implementation
multiclassLDA(train = Iris[train,-c(5)], labelsTrain = Iris[train,c(5)], priors = c(1,1,1)/3, validation = Iris[-train,])
multiclassLDA = function(train, labelsTrain, priors, validation) {
# Create means for each class
levelsClasses = levels(labelsTrain)
# Will return means for each feature for each class.
mus = apply(as.matrix(levelsClasses), 1, function(class) {
print(class)
print(labelsTrain)
print(factor(class, levels = levelsClasses))
return(mean(train[class == labelsTrain]))
})
# Extract covariance matrix for each class.
covs = apply(as.matrix(levelsClasses), 1, function(class) {
return(cov(train[class == labelsTrain]))
})
nC = apply(as.matrix(levelsClasses), 1, function(class) {
return(sum(class = labelsTrain))
})
N = sum(nC)
covM = 1/N*sum(nC*covs)
}
# Now trying my own implementation
multiclassLDA(train = Iris[train,-c(5)], labelsTrain = Iris[train,c(5)], priors = c(1,1,1)/3, validation = Iris[-train,])
multiclassLDA = function(train, labelsTrain, priors, validation) {
# Create means for each class
levelsClasses = levels(labelsTrain)
# Will return means for each feature for each class.
mus = apply(as.matrix(levelsClasses), 1, function(class) {
return(mean(train[factor(class, levels = levelsClasses) == labelsTrain]))
})
# Extract covariance matrix for each class.
covs = apply(as.matrix(levelsClasses), 1, function(class) {
return(cov(train[class == labelsTrain]))
})
nC = apply(as.matrix(levelsClasses), 1, function(class) {
return(sum(class = labelsTrain))
})
N = sum(nC)
covM = 1/N*sum(nC*covs)
}
# Trying with code and trying the LDA as instructed lda() function help example.
set.seed(123)
Iris <- data.frame(rbind(iris3[,,1], iris3[,,2], iris3[,,3]),
Sp = rep(c("s","c","v"), rep(50,3)))
train <- sample(1:150, 75)
table(Iris$Sp[train])
z <- lda(Sp ~ ., Iris, prior = c(1,1,1)/3, subset = train)
preds = predict(z, Iris[-train, ])$class
table(Iris[-train,]$Sp, preds)
# Now trying my own implementation
multiclassLDA(train = Iris[train,-c(5)], labelsTrain = Iris[train,c(5)], priors = c(1,1,1)/3, validation = Iris[-train,])
multiclassLDA = function(train, labelsTrain, priors, validation) {
# Create means for each class
levelsClasses = levels(labelsTrain)
# Will return means for each feature for each class.
mus = apply(as.matrix(levelsClasses), 1, function(class) {
return(mean(train[class == labelsTrain,]))
})
# Extract covariance matrix for each class.
covs = apply(as.matrix(levelsClasses), 1, function(class) {
return(cov(train[class == labelsTrain]))
})
nC = apply(as.matrix(levelsClasses), 1, function(class) {
return(sum(class = labelsTrain))
})
N = sum(nC)
covM = 1/N*sum(nC*covs)
}
# Now trying my own implementation
multiclassLDA(train = Iris[train,-c(5)], labelsTrain = Iris[train,c(5)], priors = c(1,1,1)/3, validation = Iris[-train,])
multiclassLDA = function(train, labelsTrain, priors, validation) {
# Create means for each class
levelsClasses = levels(labelsTrain)
# Will return means for each feature for each class.
mus = apply(as.matrix(levelsClasses), 1, function(class) {
print(factor(class, levels = levelsClasses) == labelsTrain)
return(mean(train[factor(class, levels = levelsClasses) == labelsTrain,]))
})
# Extract covariance matrix for each class.
covs = apply(as.matrix(levelsClasses), 1, function(class) {
return(cov(train[class == labelsTrain]))
})
nC = apply(as.matrix(levelsClasses), 1, function(class) {
return(sum(class = labelsTrain))
})
N = sum(nC)
covM = 1/N*sum(nC*covs)
}
# Now trying my own implementation
multiclassLDA(train = Iris[train,-c(5)], labelsTrain = Iris[train,c(5)], priors = c(1,1,1)/3, validation = Iris[-train,])
multiclassLDA = function(train, labelsTrain, priors, validation) {
# Create means for each class
levelsClasses = levels(labelsTrain)
# Will return means for each feature for each class.
mus = apply(as.matrix(levelsClasses), 1, function(class) {
print(factor(class, levels = levelsClasses) == labelsTrain)
print(mean(train[factor(class, levels = levelsClasses) == labelsTrain,]))
return(mean(train[factor(class, levels = levelsClasses) == labelsTrain,]))
})
# Extract covariance matrix for each class.
covs = apply(as.matrix(levelsClasses), 1, function(class) {
return(cov(train[class == labelsTrain]))
})
nC = apply(as.matrix(levelsClasses), 1, function(class) {
return(sum(class = labelsTrain))
})
N = sum(nC)
covM = 1/N*sum(nC*covs)
}
# Trying with code and trying the LDA as instructed lda() function help example.
set.seed(123)
Iris <- data.frame(rbind(iris3[,,1], iris3[,,2], iris3[,,3]),
Sp = rep(c("s","c","v"), rep(50,3)))
train <- sample(1:150, 75)
table(Iris$Sp[train])
z <- lda(Sp ~ ., Iris, prior = c(1,1,1)/3, subset = train)
preds = predict(z, Iris[-train, ])$class
multiclassLDA = function(train, labelsTrain, priors, validation) {
# Create means for each class
levelsClasses = levels(labelsTrain)
# Will return means for each feature for each class.
mus = apply(as.matrix(levelsClasses), 1, function(class) {
#print(factor(class, levels = levelsClasses) == labelsTrain)
print(mean(train[factor(class, levels = levelsClasses) == labelsTrain,]))
return(mean(train[factor(class, levels = levelsClasses) == labelsTrain,]))
})
# Extract covariance matrix for each class.
covs = apply(as.matrix(levelsClasses), 1, function(class) {
return(cov(train[class == labelsTrain]))
})
nC = apply(as.matrix(levelsClasses), 1, function(class) {
return(sum(class = labelsTrain))
})
N = sum(nC)
covM = 1/N*sum(nC*covs)
}
# Now trying my own implementation
multiclassLDA(train = Iris[train,-c(5)], labelsTrain = Iris[train,c(5)], priors = c(1,1,1)/3, validation = Iris[-train,])
library(Filips.ML.package)
library(MASS)
# Trying with code and trying the LDA as instructed lda() function help example.
set.seed(123)
Iris <- data.frame(rbind(iris3[,,1], iris3[,,2], iris3[,,3]),
Sp = rep(c("s","c","v"), rep(50,3)))
trainL <- sample(1:150, 75)
table(Iris$Sp[train])
z <- lda(Sp ~ ., Iris, prior = c(1,1,1)/3, subset = trainL)
preds = predict(z, Iris[-trainL, ])$class
table(Iris[-trainL,]$Sp, preds)
# Now trying my own implementation
classifs = multiclassLDA(train = Iris[trainL,-c(5)], labelsTrain = Iris[trainL,c(5)], priors = c(1,1,1)/3, validation = Iris[-trainL,-c(5)])
table(Iris[-trainL,]$Sp, classifs)
library(Filips.ML.package)
library(MASS)
# Trying with code and trying the LDA as instructed lda() function help example.
for (i in 1:10) {
set.seed(i)
Iris <- data.frame(rbind(iris3[,,1], iris3[,,2], iris3[,,3]),
Sp = rep(c("s","c","v"), rep(50,3)))
trainL <- sample(1:150, 75)
table(Iris$Sp[train])
z <- lda(Sp ~ ., Iris, prior = c(1,1,1)/3, subset = trainL)
preds = predict(z, Iris[-trainL, ])$class
# Now trying my own implementation
classifs = multiclassLDA(train = Iris[trainL,-c(5)], labelsTrain = Iris[trainL,c(5)], priors = c(1,1,1)/3, validation = Iris[-trainL,-c(5)])
table(Iris[-trainL,]$Sp, preds)
table(Iris[-trainL,]$Sp, classifs)
}
library(Filips.ML.package)
library(MASS)
# Trying with code and trying the LDA as instructed lda() function help example.
for (i in 1:10) {
set.seed(i)
Iris <- data.frame(rbind(iris3[,,1], iris3[,,2], iris3[,,3]),
Sp = rep(c("s","c","v"), rep(50,3)))
trainL <- sample(1:150, 75)
table(Iris$Sp[trainL])
z <- lda(Sp ~ ., Iris, prior = c(1,1,1)/3, subset = trainL)
preds = predict(z, Iris[-trainL, ])$class
# Now trying my own implementation
classifs = multiclassLDA(train = Iris[trainL,-c(5)], labelsTrain = Iris[trainL,c(5)], priors = c(1,1,1)/3, validation = Iris[-trainL,-c(5)])
table(Iris[-trainL,]$Sp, preds)
table(Iris[-trainL,]$Sp, classifs)
}
library(Filips.ML.package)
library(MASS)
# Trying with code and trying the LDA as instructed lda() function help example.
for (i in 1:10) {
set.seed(i)
Iris <- data.frame(rbind(iris3[,,1], iris3[,,2], iris3[,,3]),
Sp = rep(c("s","c","v"), rep(50,3)))
trainL <- sample(1:150, 75)
z <- lda(Sp ~ ., Iris, prior = c(1,1,1)/3, subset = trainL)
preds = predict(z, Iris[-trainL, ])$class
# Now trying my own implementation
classifs = multiclassLDA(train = Iris[trainL,-c(5)], labelsTrain = Iris[trainL,c(5)], priors = c(1,1,1)/3, validation = Iris[-trainL,-c(5)])
print(table(Iris[-trainL,]$Sp, preds))
print(table(Iris[-trainL,]$Sp, classifs))
}
library(Filips.ML.package)
library(MASS)
# Trying with code and trying the LDA as instructed lda() function help example.
for (i in 1:10) {
set.seed(i)
Iris <- data.frame(rbind(iris3[,,1], iris3[,,2], iris3[,,3]),
Sp = rep(c("s","c","v"), rep(50,3)))
trainL <- sample(1:150, 75)
z <- lda(Sp ~ ., Iris, prior = c(1,1,1)/3, subset = trainL)
preds = predict(z, Iris[-trainL, ])$class
# Now trying my own implementation
classifs = multiclassLDA(train = Iris[trainL,-c(5)], labelsTrain = Iris[trainL,c(5)], priors = c(1,1,1)/3, validation = Iris[-trainL,-c(5)])
print(table(Iris[-trainL,]$Sp, preds) == table(Iris[-trainL,]$Sp, classifs))
}
#https://hilaryparker.com/2014/04/29/writing-an-r-package-from-scratch/
# create("Filips.ML.package")
# setwd("./cats")
# document()
#' When update to package is done (for Filips ML package):
#' 1. Set working directory to package.
#' 2. setwd("~/Desktop/Programmeringsprojekt/Machine Learning/Filips ML package/Filips.ML.package")
#'    document()
#' 3. Set working directory to folder above and reinstall the package with updates.
#'    setwd("~/Desktop/Programmeringsprojekt/Machine Learning/Filips ML package")
#'    install("Filips.ML.package")
#'
# To use the ML functions, run the code below.
library("devtools")
library(roxygen2)
setwd("/Users/filipcornell/Desktop/Programmeringsprojekt/ML-Implementations/Filips ML package/Filips.ML.package")
document()
setwd("/Users/filipcornell/Desktop/Programmeringsprojekt/ML-Implementations/Filips ML package")
install("Filips.ML.package")
#this.dir <- dirname(parent.frame(2)$ofile) #To set it to source file location
#setwd(this.dir) #To set it to source file location
#library("Filips.ML.package")
