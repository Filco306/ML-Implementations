trControl = trainControl(method = "cv", number = 20), family = binomial())
probs_val = predict(all_model, valid, type = "prob")
Pi = as.matrix(seq(0,1,0.05))
classifications_valid = apply(Pi,1,function(pi) {
return(ifelse(probs$`1` > pi, 1, 0))
})
classifications_valid = apply(Pi,1,function(pi) {
return(ifelse(probs_val$`1` > pi, 1, 0))
})
TPR = function(predictions, labels) {
y = labels
y_hat = predictions
conf_m = table(Actual = y, Predicted = y_hat)
# True positive rate - true positives divided by all actually being positive
if (dim(conf_m)[2] == 1 && levels(labels)[2] %in% y_hat) {
return(1) # If we only have good, all are classified as positive correctly (although many are predicted incorrectly) and this is thus 1
} else if (dim(conf_m)[2] == 1 && levels(labels)[1] %in% y_hat) {
return(0)
} else {
TP = conf_m[2,2] # True positives
N_plus = sum(conf_m[2,])
return(TP/N_plus)
}
}
FPR = function(predictions, labels) {
y = labels
y_hat = predictions
conf_m = table(Actual = y, Predicted = y_hat)
# False positive rate - false positives divided by all actually being negative
if (dim(conf_m)[2] == 1 && levels(labels)[2] %in% y_hat) {
return(1) # If we only have "good", it means all are classified as positive, and the false positives is thus as many as the number of false ones. Thus, the FP/N_minus = 1
} else if (dim(conf_m)[2] == 1 && levels(labels)[1] %in% y_hat) {
return(0) # If we only have bad, we do not have a single false positive as there are no positives. Hence, 0
} else {
FP = conf_m[1,2]
N_minus = sum(conf_m[1,])
return(FP/N_minus)
}
}
TPRs = apply(classifications_valid, 2, function(col) {
return(TPR(predictions = as.factor(col), labels = valid$y))
})
FPRs = apply(classifications_valid, 2, function(col) {
return(FPR(predictions = as.factor(col), labels = valid$y))
})
#TPRs =
plot(x=FPRs, y = TPRs, type = "l", main ="Logistic regression ROC curve")
AUC = function(TPR, FPR) {
# TPR is y, FPR is x
# Order after FPR
xInd = order(FPR)
x = FPR[xInd]
y = TPR[xInd]
area = 0
for (i in 2:length(TPR)) {
area = (x[i]-x[i-1])*(y[i] + y[i-1])/2 + area
}
return(area)
}
AUC(TPRs, FPRs)
prob_threshold = Pi[which.max(TPRs - FPRs),]
test_preds = predict(all_model, test, type = "prob")
classifs_test = as.factor(ifelse(test_preds[,2] > prob_threshold, 1,0))
table(test$y, classifs_test)
1 - sum(diag(table(test$y, classifs_test)))/sum(table(test$y, classifs_test))
n=dim(data)[1]
set.seed(12345)
id=sample(1:n, floor(n*0.6))
train=data[id,]
id1=setdiff(1:n, id)
set.seed(12345)
id2=sample(id1, floor(n*0.2))
valid=data[id2,]
id3=setdiff(id1,id2)
test=data[id3,]
all_model = fit <- train(
x = train[,-c(9)], y = train[,9], method = "glm",
trControl = trainControl(method = "cv", number = 20), family = binomial())
probs_val = predict(all_model, valid, type = "prob")
#library(Filips.ML.package)
Pi = as.matrix(seq(0,1,0.05))
classifications_valid = apply(Pi,1,function(pi) {
return(ifelse(probs_val$`1` > pi, 1, 0))
})
TPR = function(predictions, labels) {
y = labels
y_hat = predictions
conf_m = table(Actual = y, Predicted = y_hat)
# True positive rate - true positives divided by all actually being positive
if (dim(conf_m)[2] == 1 && levels(labels)[2] %in% y_hat) {
return(1) # If we only have good, all are classified as positive correctly (although many are predicted incorrectly) and this is thus 1
} else if (dim(conf_m)[2] == 1 && levels(labels)[1] %in% y_hat) {
return(0)
} else {
TP = conf_m[2,2] # True positives
N_plus = sum(conf_m[2,])
return(TP/N_plus)
}
}
FPR = function(predictions, labels) {
y = labels
y_hat = predictions
conf_m = table(Actual = y, Predicted = y_hat)
# False positive rate - false positives divided by all actually being negative
if (dim(conf_m)[2] == 1 && levels(labels)[2] %in% y_hat) {
return(1) # If we only have "good", it means all are classified as positive, and the false positives is thus as many as the number of false ones. Thus, the FP/N_minus = 1
} else if (dim(conf_m)[2] == 1 && levels(labels)[1] %in% y_hat) {
return(0) # If we only have bad, we do not have a single false positive as there are no positives. Hence, 0
} else {
FP = conf_m[1,2]
N_minus = sum(conf_m[1,])
return(FP/N_minus)
}
}
TPRs = apply(classifications_valid, 2, function(col) {
return(TPR(predictions = as.factor(col), labels = valid$y))
})
FPRs = apply(classifications_valid, 2, function(col) {
return(FPR(predictions = as.factor(col), labels = valid$y))
})
#TPRs =
plot(x=FPRs, y = TPRs, type = "l", main ="Logistic regression ROC curve")
AUC = function(TPR, FPR) {
# TPR is y, FPR is x
# Order after FPR
xInd = order(FPR)
x = FPR[xInd]
y = TPR[xInd]
area = 0
for (i in 2:length(TPR)) {
area = (x[i]-x[i-1])*(y[i] + y[i-1])/2 + area
}
return(area)
}
AUC(TPRs, FPRs)
prob_threshold = Pi[which.max(TPRs - FPRs),]
test_preds = predict(all_model, test, type = "prob")
classifs_test = as.factor(ifelse(test_preds[,2] > prob_threshold, 1,0))
table(test$y, classifs_test)
1 - sum(diag(table(test$y, classifs_test)))/sum(table(test$y, classifs_test))
n=dim(data)[1]
set.seed(12345)
id=sample(1:n, floor(n*0.4))
train=data[id,]
id1=setdiff(1:n, id)
set.seed(12345)
id2=sample(id1, floor(n*0.3))
valid=data[id2,]
id3=setdiff(id1,id2)
test=data[id3,]
all_model = fit <- train(
x = train[,-c(9)], y = train[,9], method = "glm",
trControl = trainControl(method = "cv", number = 20), family = binomial())
probs_val = predict(all_model, valid, type = "prob")
#library(Filips.ML.package)
Pi = as.matrix(seq(0,1,0.05))
classifications_valid = apply(Pi,1,function(pi) {
return(ifelse(probs_val$`1` > pi, 1, 0))
})
TPR = function(predictions, labels) {
y = labels
y_hat = predictions
conf_m = table(Actual = y, Predicted = y_hat)
# True positive rate - true positives divided by all actually being positive
if (dim(conf_m)[2] == 1 && levels(labels)[2] %in% y_hat) {
return(1) # If we only have good, all are classified as positive correctly (although many are predicted incorrectly) and this is thus 1
} else if (dim(conf_m)[2] == 1 && levels(labels)[1] %in% y_hat) {
return(0)
} else {
TP = conf_m[2,2] # True positives
N_plus = sum(conf_m[2,])
return(TP/N_plus)
}
}
FPR = function(predictions, labels) {
y = labels
y_hat = predictions
conf_m = table(Actual = y, Predicted = y_hat)
# False positive rate - false positives divided by all actually being negative
if (dim(conf_m)[2] == 1 && levels(labels)[2] %in% y_hat) {
return(1) # If we only have "good", it means all are classified as positive, and the false positives is thus as many as the number of false ones. Thus, the FP/N_minus = 1
} else if (dim(conf_m)[2] == 1 && levels(labels)[1] %in% y_hat) {
return(0) # If we only have bad, we do not have a single false positive as there are no positives. Hence, 0
} else {
FP = conf_m[1,2]
N_minus = sum(conf_m[1,])
return(FP/N_minus)
}
}
TPRs = apply(classifications_valid, 2, function(col) {
return(TPR(predictions = as.factor(col), labels = valid$y))
})
FPRs = apply(classifications_valid, 2, function(col) {
return(FPR(predictions = as.factor(col), labels = valid$y))
})
#TPRs =
plot(x=FPRs, y = TPRs, type = "l", main ="Logistic regression ROC curve")
AUC = function(TPR, FPR) {
# TPR is y, FPR is x
# Order after FPR
xInd = order(FPR)
x = FPR[xInd]
y = TPR[xInd]
area = 0
for (i in 2:length(TPR)) {
area = (x[i]-x[i-1])*(y[i] + y[i-1])/2 + area
}
return(area)
}
AUC(TPRs, FPRs)
prob_threshold = Pi[which.max(TPRs - FPRs),]
test_preds = predict(all_model, test, type = "prob")
classifs_test = as.factor(ifelse(test_preds[,2] > prob_threshold, 1,0))
table(test$y, classifs_test)
1 - sum(diag(table(test$y, classifs_test)))/sum(table(test$y, classifs_test))
library("devtools")
library("devtools")
library(roxygen2)
setwd("/Users/filipcornell/Desktop/Programmeringsprojekt/ML-Implementations/Filips ML package/Filips.ML.package")
document()
setwd("/Users/filipcornell/Desktop/Programmeringsprojekt/ML-Implementations/Filips ML package")
install("Filips.ML.package")
help(neuralnet)
#https://hilaryparker.com/2014/04/29/writing-an-r-package-from-scratch/
# create("Filips.ML.package")
# setwd("./cats")
# document()
#' When update to package is done (for Filips ML package):
#' 1. Set working directory to package.
#' 2. setwd("~/Desktop/Programmeringsprojekt/Machine Learning/Filips ML package/Filips.ML.package")
#'    document()
#' 3. Set working directory to folder above and reinstall the package with updates.
#'    setwd("~/Desktop/Programmeringsprojekt/Machine Learning/Filips ML package")
#'    install("Filips.ML.package")
#'
# To use the ML functions, run the code below.
library("devtools")
library(roxygen2)
setwd("/Users/filipcornell/Desktop/Programmeringsprojekt/ML-Implementations/Filips ML package/Filips.ML.package")
document()
setwd("/Users/filipcornell/Desktop/Programmeringsprojekt/ML-Implementations/Filips ML package")
install("Filips.ML.package")
#this.dir <- dirname(parent.frame(2)$ofile) #To set it to source file location
#setwd(this.dir) #To set it to source file location
#library("Filips.ML.package")
i = 1
nn <- neuralnet(Sin~Var, data=train,hidden=10,threshold = i/1000, startweights = winit)# Your code here)
winit <- runif(50,-1,1)
nn <- neuralnet(Sin~Var, data=train,hidden=10,threshold = i/1000, startweights = winit)# Your code here)
MSE = function(y,y_hat) {
return(mean((y - y_hat)^2))
}
library(neuralnet)
set.seed(1234567890)
Var <- runif(50, 0, 10)
trva <- data.frame(Var, Sin=sin(Var))
tr <- trva[1:25,] # Training
va <- trva[26:50,] # Validation
# Random initialization of the weights in the interval [-1, 1]
winit <- runif(50,-1,1)
nn <- neuralnet(Sin~Var, data=tr,hidden=10,threshold = i/1000, startweights = winit)# Your code here)
help(compute)
val_preds = compute(nn, va)$net.result
compute(nn, va$Var)$net.result
val_preds = compute(nn, va$Var)$net.result
MSE_val = as.numeric()
MSE_train = as.numeric()
for(i in 1:10) {
nn <- neuralnet(Sin~Var, data=tr,hidden=10,threshold = i/1000, startweights = winit)# Your code here)
# Your code here
tr_preds = compute(nn, tr$Var)$net.result
val_preds = compute(nn, va$Var)$net.result
MSE_val[i] = MSE(va$Var, val_preds)
MSE_train[i] = MSE(tr$Var, tr_preds)
}
# Which error is the best?
plot(x=seq(1,10,1), y=MSE_val, main = "MSE for training and validation set", xlab = "Threshold", ylab = "MSE", type = "l", lwd = 3, col = "red")
lines(x=seq(1,10,1), y = MSE_train, lwd = 3, col = "blue")
# Which error is the best?
plot(x=seq(1,10,1), y=MSE_val, main = "MSE for training and validation set", xlab = "Threshold", ylab = "MSE", type = "l", lwd = 3, col = "red", ylim=c(0,max(MSE_train, MSE_val)))
lines(x=seq(1,10,1), y = MSE_train, lwd = 3, col = "blue")
MSE = function(y,y_hat) {
return(mean((y - y_hat)^2))
}
library(neuralnet)
set.seed(1234567890)
Var <- runif(50, 0, 10)
trva <- data.frame(Var, Sin=sin(Var))
tr <- trva[1:25,] # Training
va <- trva[26:50,] # Validation
# Random initialization of the weights in the interval [-1, 1]
winit <- runif(50,-1,1)
MSE_val = as.numeric()
MSE_train = as.numeric()
for(i in 1:10) {
nn <- neuralnet(Sin~Var, data=tr,hidden=10,threshold = i/1000, startweights = winit)# Your code here)
# Your code here
tr_preds = compute(nn, tr$Var)$net.result
val_preds = compute(nn, va$Var)$net.result
MSE_val[i] = MSE(va$Var, val_preds)
MSE_train[i] = MSE(tr$Var, tr_preds)
}
# Which error is the best?
plot(x=seq(1,10,1), y=MSE_val, main = "MSE for training and validation set", xlab = "Threshold", ylab = "MSE", type = "l", lwd = 3, col = "red", ylim=c(0,max(MSE_train, MSE_val)))
lines(x=seq(1,10,1), y = MSE_train, lwd = 3, col = "blue")
for(i in 1:10) {
nn <- neuralnet(Sin~Var, data=tr,hidden=10,threshold = i/1000, startweights = winit)# Your code here)
# Your code here
tr_preds = compute(nn, tr$Var)$net.result
val_preds = compute(nn, va$Var)$net.result
MSE_val[i] = MSE(va$Var, val_preds[,1])
MSE_train[i] = MSE(tr$Var, tr_preds[,1])
}
# Which error is the best?
plot(x=seq(1,10,1), y=MSE_val, main = "MSE for training and validation set", xlab = "Threshold", ylab = "MSE", type = "l", lwd = 3, col = "red", ylim=c(0,max(MSE_train, MSE_val)))
lines(x=seq(1,10,1), y = MSE_train, lwd = 3, col = "blue")
# Reuse function created in lab 1
MSE = function(y, y_hat) {
n = length(y)
return((1/n)*sum((y - y_hat)^2))
}
#install.packages("neuralnet")
library(neuralnet)
set.seed(1234567890)
Var <- runif(50, 0, 10)
trva <- data.frame(Var, Sin=sin(Var))
tr <- trva[1:25,] # Training
va <- trva[26:50,] # Validation
# Random initialization of the weights in the interval [-1, 1]
winit <- runif(50, min = -1, max = 1)
thr = as.numeric()
MSEsValidation = as.numeric()
MSEsTrain = as.numeric()
for (i in 1:10) {
thr[i] = i/1000
nn <- neuralnet(Sin ~ Var, data = tr,
threshold = thr[i], startweights = winit, hidden = 10)
# Use validation set here to get error
yValidation = compute(nn, va$Var)$net.result
MSEsValidation[i] = MSE(y = va$Sin, y_hat = yValidation[,1])
yTrain = compute(nn, tr$Var)$net.result
MSEsTrain[i] = MSE(y = tr$Sin, y_hat = yTrain[,1])
}
plot(x = thr, y = MSEsValidation, type = "l",
lwd = 3, xlab = "Threshold",
ylab = "MSE", main = "MSE on validation set",
ylim = c(min(MSEsTrain, MSEsValidation), max(MSEsTrain, MSEsValidation)))
lines(x = thr, y = MSEsTrain, col = "blue", lwd = 3)
plot(x = thr, y = MSEsValidation, type = "l",
lwd = 3, xlab = "Threshold",
ylab = "MSE", main = "MSE on validation set",
ylim = c(min(MSEsTrain, MSEsValidation), max(MSEsTrain, MSEsValidation)))
MSEsValidation
MSE = function(y,y_hat) {
return(mean((y - y_hat)^2))
}
library(neuralnet)
set.seed(1234567890)
Var <- runif(50, 0, 10)
trva <- data.frame(Var, Sin=sin(Var))
tr <- trva[1:25,] # Training
va <- trva[26:50,] # Validation
# Random initialization of the weights in the interval [-1, 1]
winit <- runif(50,-1,1)
MSE_val = as.numeric()
MSE_train = as.numeric()
for(i in 1:10) {
#nn <- neuralnet(Sin ~ Var, data = tr,
#              threshold = thr[i], startweights = winit, hidden = 10)
nn <- neuralnet(Sin~Var, data=tr,hidden=10,threshold = (i/1000), startweights = winit)# Your code here)
# Your code here
tr_preds = compute(nn, tr$Var)$net.result
val_preds = compute(nn, va$Var)$net.result
MSE_val[i] = MSE(va$Var, val_preds[,1])
MSE_train[i] = MSE(tr$Var, tr_preds[,1])
}
MSE_val
MSE = function(y, y_hat) {
n = length(y)
return((1/n)*sum((y - y_hat)^2))
}
library(neuralnet)
set.seed(1234567890)
Var <- runif(50, 0, 10)
trva <- data.frame(Var, Sin=sin(Var))
tr <- trva[1:25,] # Training
va <- trva[26:50,] # Validation
# Random initialization of the weights in the interval [-1, 1]
winit <- runif(50,-1,1)
MSE_val = as.numeric()
MSE_train = as.numeric()
for(i in 1:10) {
#nn <- neuralnet(Sin ~ Var, data = tr,
#              threshold = thr[i], startweights = winit, hidden = 10)
nn <- neuralnet(Sin~Var, data=tr,hidden=10,threshold = (i/1000), startweights = winit)# Your code here)
# Your code here
tr_preds = compute(nn, tr$Var)$net.result
val_preds = compute(nn, va$Var)$net.result
MSE_val[i] = MSE(va$Var, val_preds[,1])
MSE_train[i] = MSE(tr$Var, tr_preds[,1])
}
MSE_val
MSE = function(y, y_hat) {
n = length(y)
return((1/n)*sum((y - y_hat)^2))
}
library(neuralnet)
set.seed(1234567890)
Var <- runif(50, 0, 10)
trva <- data.frame(Var, Sin=sin(Var))
tr <- trva[1:25,] # Training
va <- trva[26:50,] # Validation
# Random initialization of the weights in the interval [-1, 1]
winit <- runif(50,-1,1)
MSE_val = as.numeric()
MSE_train = as.numeric()
for(i in 1:10) {
nn <- neuralnet(Sin ~ Var, data = tr,
threshold = (i/1000), startweights = winit, hidden = 10)
#nn <- neuralnet(Sin~Var, data=tr,hidden=10,threshold = (i/1000), startweights = winit)# Your code here)
# Your code here
tr_preds = compute(nn, tr$Var)$net.result
val_preds = compute(nn, va$Var)$net.result
MSE_val[i] = MSE(va$Var, val_preds[,1])
MSE_train[i] = MSE(tr$Var, tr_preds[,1])
}
MSE_val
MSE = function(y, y_hat) {
n = length(y)
return((1/n)*sum((y - y_hat)^2))
}
library(neuralnet)
set.seed(1234567890)
Var <- runif(50, 0, 10)
trva <- data.frame(Var, Sin=sin(Var))
tr <- trva[1:25,] # Training
va <- trva[26:50,] # Validation
# Random initialization of the weights in the interval [-1, 1]
winit <- runif(50,-1,1)
MSE_val = as.numeric()
MSE_train = as.numeric()
for(i in 1:10) {
nn <- neuralnet(Sin ~ Var, data = tr,
threshold = (i/1000), startweights = winit, hidden = 10)
#nn <- neuralnet(Sin ~ Var, data=tr,hidden=10,threshold = (i/1000), startweights = winit)# Your code here)
# Your code here
#tr_preds = compute(nn, tr$Var)$net.result
#val_preds = compute(nn, va$Var)$net.result
#MSE_val[i] = MSE(va$Var, val_preds[,1])
#MSE_train[i] = MSE(tr$Var, tr_preds[,1])
yValidation = compute(nn, va$Var)$net.result
MSE_val[i] = MSE(y = va$Sin, y_hat = yValidation[,1])
yTrain = compute(nn, tr$Var)$net.result
MSE_train[i] = MSE(y = tr$Sin, y_hat = yTrain[,1])
}
# Which error is the best?
plot(x=seq(1,10,1), y=MSE_val, main = "MSE for training and validation set", xlab = "Threshold", ylab = "MSE", type = "l", lwd = 3, col = "red", ylim=c(0,max(MSE_train, MSE_val)))
lines(x=seq(1,10,1), y = MSE_train, lwd = 3, col = "blue")
MSE = function(y, y_hat) {
n = length(y)
return((1/n)*sum((y - y_hat)^2))
}
library(neuralnet)
set.seed(1234567890)
Var <- runif(50, 0, 10)
trva <- data.frame(Var, Sin=sin(Var))
tr <- trva[1:25,] # Training
va <- trva[26:50,] # Validation
# Random initialization of the weights in the interval [-1, 1]
winit <- runif(50,-1,1)
MSE_val = as.numeric()
MSE_train = as.numeric()
for(i in 1:10) {
nn <- neuralnet(Sin ~ Var, data = tr,
threshold = (i/1000), startweights = winit, hidden = 10)
#nn <- neuralnet(Sin ~ Var, data=tr,hidden=10,threshold = (i/1000), startweights = winit)# Your code here)
# Your code here
#tr_preds = compute(nn, tr$Var)$net.result
val_preds = compute(nn, va$Var)$net.result
MSE_val[i] = MSE(va$Sin, val_preds[,1])
MSE_train[i] = MSE(tr$Sin, tr_preds[,1])
}
# Which error is the best?
plot(x=seq(1,10,1), y=MSE_val, main = "MSE for training and validation set", xlab = "Threshold", ylab = "MSE", type = "l", lwd = 3, col = "red", ylim=c(0,max(MSE_train, MSE_val)))
lines(x=seq(1,10,1), y = MSE_train, lwd = 3, col = "blue")
MSE = function(y, y_hat) {
n = length(y)
return((1/n)*sum((y - y_hat)^2))
}
library(neuralnet)
set.seed(1234567890)
Var <- runif(50, 0, 10)
trva <- data.frame(Var, Sin=sin(Var))
tr <- trva[1:25,] # Training
va <- trva[26:50,] # Validation
# Random initialization of the weights in the interval [-1, 1]
winit <- runif(50,-1,1)
MSE_val = as.numeric()
MSE_train = as.numeric()
for(i in 1:10) {
nn <- neuralnet(Sin ~ Var, data = tr,
threshold = (i/1000), startweights = winit, hidden = 10)
#nn <- neuralnet(Sin ~ Var, data=tr,hidden=10,threshold = (i/1000), startweights = winit)# Your code here)
# Your code here
tr_preds = compute(nn, tr$Var)$net.result
val_preds = compute(nn, va$Var)$net.result
MSE_val[i] = MSE(va$Sin, val_preds[,1])
MSE_train[i] = MSE(tr$Sin, tr_preds[,1])
}
# Which error is the best?
plot(x=(seq(1,10,1)/1000), y=MSE_val, main = "MSE for training and validation set", xlab = "Threshold", ylab = "MSE", type = "l", lwd = 3, col = "red", ylim=c(0,max(MSE_train, MSE_val)))
lines(x=(seq(1,10,1)/1000), y = MSE_train, lwd = 3, col = "blue")
